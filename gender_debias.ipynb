{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gender-debias.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DCSaunders/gender-debias/blob/master/gender_debias.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UN_D-dlktVU",
        "colab_type": "text"
      },
      "source": [
        "# Reducing gender bias in neural machine translation as a domain adaptation problem\n",
        "This notebook walks through the paper [Reducing Gender Bias in Neural Machine Translation as a Domain Adaptation Problem](https://arxiv.org/abs/2004.04498):\n",
        "\n",
        "\n",
        "\n",
        "*   What the gender bias problem is in Neural Machine Translation (NMT), and how we can measure it.\n",
        "*   How to debias an NMT model by *domain adaptation*: fine-tuning on a new, small, debiased dataset\n",
        "*   How to avoid *catastrophic forgetting*: debiasing the model while keeping its general translation ability\n",
        "*   How to use the debiased model for *lattice rescoring*: \"correcting\" words with the wrong gender inflection in machine translations.\n",
        "\n",
        "This notebook walks through steps to reproduce our primary experiments, including tool installation, our baseline models and adaptation data.\n",
        "<!-- \n",
        "Otherwise, to reproduce locally:\n",
        "\n",
        "* Download the desired language pair files from the Google Drive https://drive.google.com/drive/folders/1XaWbXQQ8icZZE__fPBOj4dC24oiA3Qtv?usp=sharing  (For example, if you just want to reproduce our English-German experiments, you can ignore the folders ending with \"enes\" or \"enhe\" but should take everything else). These folders include our BPE vocabulary mappings and scripts to convert between plaintext and id-maps.\n",
        "* Download Tensor2Tensor version 1.4.3 or more recent if you wish to use our baseline models for fine-tuning. The main repo is https://github.com/tensorflow/tensor2tensor - if you want to use our EWC implementation, check out https://github.com/DCSaunders/tensor2tensor/tree/ewc.v1.5\n",
        "* Download OpenFST http://www.openfst.org/twiki/bin/view/FST/WebHome if you wish to generate translation lattices for lattice rescoring\n",
        "* Download SGNMT https://github.com/ucam-smt/sgnmt if you wish to use our setup for decoding, including implementations for rescoring using OpenFST lattices\n",
        "* Download sacrebleu https://github.com/mjpost/sacreBLEU/ for reproducible BLEU score evaluation\n",
        "* Download WinoMT https://github.com/gabrielStanovsky/mt_gender for gender bias challenge set evaluation -->\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkhIGZfb7KQ4",
        "colab_type": "text"
      },
      "source": [
        "## First set up all tools used in this notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dw0oSwknN1Mg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# install miniconda\n",
        "! wget https://repo.continuum.io/miniconda/Miniconda3-4.5.4-Linux-x86_64.sh && bash Miniconda3-4.5.4-Linux-x86_64.sh -bfp /usr/local\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocrI1nZ37JUD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Install tools tensor2tensor dependency.   \n",
        "# Restart runtime if prompted\n",
        "!conda create -y -n tf_env_main pip python=3.6\n",
        "! source activate tf_env_main\n",
        "!pip install cloudpickle==1.2.0\n",
        "\n",
        "!pip install -q -U tensor2tensor\n",
        "# will require TF version 1.15.2 for tensor2tensor\n",
        "# restart if prompted \n",
        "import sys\n",
        "if 'google.colab' in sys.modules: # Colab-only TensorFlow version selector\n",
        "  %tensorflow_version 1.15.2\n",
        "import tensorflow as tf\n",
        "import tensor2tensor\n",
        "!pip install -q openfst-python\n",
        "! pip install sacrebleu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcUpcxSk9QEP",
        "colab_type": "text"
      },
      "source": [
        "Set up environment for SGNMT decoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyKmqiq_9hjB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bash\n",
        "conda install -y -q -c conda-forge openfst\n",
        "conda create -y -n sgnmt_env pip python=3.6\n",
        "conda install --name sgnmt_env -y -q -c conda-forge openfst\n",
        "source activate sgnmt_env\n",
        "git clone https://github.com/ucam-smt/sgnmt.git\n",
        "pip install -q openfst-python\n",
        "conda install --name sgnmt_env -y -q -f numpy pyyaml  scipy\"==1.0.0\"  tensorflow\n",
        "conda install --name sgnmt_env -y -q -c conda-forge openfst\n",
        "\n",
        "pip install tensor2tensor\n",
        "\n",
        "FSTPATH=/usr/local/lib/python3.6/dist-packages/openfst_python\n",
        "PYTHONPATH=$FSTPATH:$PYTHONPATH\n",
        "## verify\n",
        "echo $PYTHONPATH\n",
        "python sgnmt/decode.py --run_diagnostics\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjWjXgSW8FKe",
        "colab_type": "text"
      },
      "source": [
        "Check that installations behave correctly inside python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccNsKtir9HqP",
        "colab_type": "text"
      },
      "source": [
        "Set up gender bias evaluation requirements\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nf2bWQOx9HO5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bash\n",
        "echo \"git clone fast_align\"\n",
        "git clone https://github.com/clab/fast_align.git\n",
        "cd fast_align\n",
        "mkdir -p build\n",
        "cd build\n",
        "echo \"cmake\"\n",
        "cmake ..\n",
        "echo \"make\"\n",
        "make\n",
        "echo \"test fast align\"\n",
        "/content/fast_align/build/fast_align\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WesY74dgDeoH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bash\n",
        "git clone https://github.com/gabrielStanovsky/mt_gender.git\n",
        "cd mt_gender\n",
        "FAST_ALIGN=/content/fast_align/\n",
        "./install.sh"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMqGGvDA7wtk",
        "colab_type": "text"
      },
      "source": [
        "The files and scripts needed to run these GDB are available from a shared Google drive which you will need to link to your own Google drive, as follows:\n",
        "\n",
        "1. In a seperate browser window, go to  https://drive.google.com/drive/folders/1XaWbXQQ8icZZE__fPBOj4dC24oiA3Qtv?usp=sharing \n",
        "1. Select `gender-debias-walkthrough`\n",
        "1. Click on `Add shortcut to Drive`\n",
        "\n",
        "You should now see a link to the folder `gender-debias-walkthrough` in `My Drive`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOo_gZoR7mbC",
        "colab_type": "text"
      },
      "source": [
        "Mount your google drive so this folder is visible in this notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHnJyCKA7q1u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "! ls -d /content/drive/My\\ Drive\n",
        "! ls /content/drive/My\\ Drive/gender-debias-walkthrough"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25_Rkxp1vzqy",
        "colab_type": "text"
      },
      "source": [
        "# The gender bias problem\n",
        "\n",
        "Let's say we want to train a neural network to translate from English to German. We can do this by showing the model lots of examples of real English sentences and their German translations.\n",
        "\n",
        "But we have to get those example translations from somewhere. News reports, political speeches, Wikipedia and TED talks are common sources. Society being what it is, these tend to have more examples of men than of women.\n",
        "\n",
        "In fact we can count the number of sentences in the English side of the dataset which contain masculine words like *he, him, his, man, men*, etc, and do the same for the equivalent feminine words. We find there are about twice as many masculine sentences.\n",
        "\n",
        "German, like many languages, has grammatical gender. *The doctor* should be translated into German as *Der Arzt* if the doctor is male,  *Die &Auml;rztin* if the doctor is female. But because of data bias, machine translation models often get this wrong.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMtV5_pcxBai",
        "colab_type": "text"
      },
      "source": [
        "# Can we measure gender bias in translation?\n",
        "\n",
        "\n",
        "Yes! [Stanovsky et al (2019)](https://www.aclweb.org/anthology/P19-1164.pdf)  recently developed the [WinoMT framework](https://github.com/gabrielStanovsky/mt_gender) for this purpose. WinoMT comes with a set of 3888 sentences, for example:\n",
        "\n",
        "*The **physician** told the nurse that **he** had been busy.*\n",
        "\n",
        "*The **physician** told the nurse that **she** had been busy.*\n",
        "\n",
        "\n",
        "\n",
        "*   Each sentence has a primary entity -- *the physician* in these examples.\n",
        "*  Each primary entity is coreferent with a pronoun -- *he* in the first example, *she* in the second.\n",
        "* WinoMT is gender-balanced: there are the same total number of male and female physicians (and every other kind of primary entity)\n",
        "\n",
        "If you translate these 3888 sentences from English to one of 8 supported languages with grammatical gender, WinoMT provides an automatic evaluation framework to see how much gender bias is in those translations:\n",
        "\n",
        "*   It automatically aligns your translations with the English input to find your translated version of the primary entity\n",
        "*   Using morphological analysis tools, WinoMT extracts the translated primary entity's grammatical gender\n",
        "*   The objective is to achieve the correct grammatical gender for each sentence. WinoMT reports this **accuracy** as a percentage over all sentences.\n",
        "*   Because WinoMT has the same number of male and female sentences, it can also report the difference in translation quality between them as **∆G**, the difference in [F1 score](https://en.wikipedia.org/wiki/F1_score) between sentences with male and female entities. If ∆G is above 0, it gets the correct gender for male entities more frequently than for female entities.\n",
        "\n",
        "Stanovsky et al report results on several commercial translation systems, like Google and Bing translate. We can also use our own baseline model to translate the WinoMT set and evaluate it to see the same problem.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F245X0snksh6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bash\n",
        "BASEDIR='/content/drive/My Drive/gender-debias-walkthrough/'\n",
        "WINOMT_PREP_SCRIPT=\"$BASEDIR/scripts/prep_for_winomt_eval.sh\"\n",
        "BASELINE_HYP=\"$BASEDIR/data_ende/winomt.ende.baseline-hyp.detok\"\n",
        "lang=de \n",
        "winomt_logs=winomtout\n",
        "mkdir -p $winomt_logs\n",
        "bash \"$WINOMT_PREP_SCRIPT\" \"$BASELINE_HYP\" $lang /content/mt_gender\n",
        "cd /content/mt_gender/src\n",
        "export FAST_ALIGN_BASE=/content/fast_align/\n",
        "bash ../scripts/evaluate_all_languages.sh ../data/aggregates/en.txt $winomt_logs\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D32bzB82REm9",
        "colab_type": "text"
      },
      "source": [
        "# What can we do about gender bias in machine translation?\n",
        "There's been increasing interest in reducing gender bias in language processing tasks in the last year or two. Proposed solutions include trying to [debias parts of the model before or during training](https://www.aclweb.org/anthology/W19-3821.pdf), or [producing multiple differently gendered translations in ambiguous cases](https://www.blog.google/products/translate/reducing-gender-bias-google-translate/). \n",
        "\n",
        "Our approach is based on domain adaptation: we want to adapt the model to a domain which has no gender bias, encouraging it to forget about any previously seen data bias."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0V8CGauUXdEx",
        "colab_type": "text"
      },
      "source": [
        "# Adapting a model to balanced data\n",
        "\n",
        "We can do this very quickly with a tiny dataset which we create in English and translate into target languages like German. The dataset contains sentences pairs like:\n",
        "\n",
        "\n",
        "*The doctor finished his work. | Der Arzt beendete seine Arbeit.*\n",
        "\n",
        "*The doctor finished her work. | Die Ärztin beendete ihre Arbeit.*\n",
        "\n",
        "And so on for 194 professions, for a total of just 388 sentences.\n",
        "\n",
        "Since the adaptation dataset is very small, we only iterate over it a few times before convergence. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vo7KUrn_Wu88",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bash\n",
        "source activate tf_env_main\n",
        "\n",
        "BASEDIR='/content/drive/My Drive/gender-debias-walkthrough/'\n",
        "SRC_DIR=\"$BASEDIR/baseline_ende\"\n",
        "T2T_USR_DIR=\"$BASEDIR/t2t-usr\"\n",
        "T2T=\"/tensorflow-1.15.2/python3.6/tensor2tensor/bin/\"\n",
        "TF=\"/tensorflow-1.15.2/python3.6/bin\"\n",
        "DATA_DIR=\"$BASEDIR/data_ende/handcrafted_ende\"\n",
        "\n",
        "BASE_STEPS=300000\n",
        "adapt_steps=4\n",
        "TRAIN_STEPS==$(( $BASE_STEPS + $adapt_steps ))\n",
        "batch_size=4096\n",
        "\n",
        "model_dir=model/ft/ende/\n",
        "mkdir -p $model_dir\n",
        "\n",
        "# make a local copy of T2T_USR_DIR in t2t-usr\n",
        "cp \"$SRC_DIR\"/model* $model_dir/\n",
        "cp \"$SRC_DIR\"/checkpoint $model_dir/\n",
        "\n",
        "export PYTHONPATH=\"$T2T:$T2T_USR_DIR:$TF:$PYTHONPATH\" \n",
        "t2t-trainer \\\n",
        " --data_dir=\"$DATA_DIR\" \\\n",
        " --problem=translate_generic_existing_vocab --hparams_set=transformer_base \\\n",
        " --t2t_usr_dir=\"$T2T_USR_DIR\" --output_dir=$model_dir --model=transformer \\\n",
        " --schedule=train --train_steps=300004 --keep_checkpoint_max=1  \\\n",
        " --hparams=\"batch_size=$batch_size\" \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xP_Ibi0YKrQ",
        "colab_type": "text"
      },
      "source": [
        "# Inference with the adapted model\n",
        "Having fine-tuned the model on the balanced dataset, we can use it to translate the WinoMT test set again, using SGNMT to handle beam search decoding. We first create a config file which specifies what we want to decode and how we want to decode.\n",
        "\n",
        "To decode the WinoMT test set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEYS8KjGBLPb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bash\n",
        "\n",
        "BASEDIR=/content/drive/My\\ Drive/gender-debias-walkthrough\n",
        "SRC_VOCAB_SIZE=$(wc -l < \"$BASEDIR/data_ende/handcrafted_ende/vocab.src\")\n",
        "TRG_VOCAB_SIZE=$(wc -l < \"$BASEDIR/data_ende/handcrafted_ende/vocab.trg\")\n",
        "T2T_USR_DIR=\"$BASEDIR/t2t-usr\"\n",
        "SRC_TEST=\"$BASEDIR/data_ende/winomt.ende.src.ids\"\n",
        "model_dir=model/ft/ende/\n",
        "decode_dir=decode_adapted_winomt\n",
        "mkdir -p $decode_dir\n",
        "\n",
        "config_file=$decode_dir/decode.ini\n",
        "echo \"verbosity: debug\n",
        "predictors: t2t\n",
        "src_test: \"$SRC_TEST\"\n",
        "decoder: beam\n",
        "beam: 4\n",
        "t2t_model: transformer\n",
        "t2t_hparams_set: transformer_base\n",
        "t2t_problem: translate_generic_existing_vocab\n",
        "pred_src_vocab_size: \"$SRC_VOCAB_SIZE\"\n",
        "pred_trg_vocab_size: \"$TRG_VOCAB_SIZE\"\n",
        "indexing_scheme: t2t\n",
        "t2t_usr_dir: \"$T2T_USR_DIR\"\n",
        "t2t_unk_id: 3\n",
        "output_path: \"$decode_dir\"/output.ids\n",
        "t2t_checkpoint_dir: \"$model_dir\"\n",
        "outputs: text\" > $config_file\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHPOUYD1At-o",
        "colab_type": "text"
      },
      "source": [
        "Then we activate the decoding environment and run decoding.\n",
        "\n",
        "Warning: the  ```--range=1:1``` option decodes only the first sentence. Removing the range option will default to decoding the entire test set, which could take a long time.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKH0DmKxAtzH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bash \n",
        "source activate sgnmt_env\n",
        "BASEDIR=/content/drive/My\\ Drive/gender-debias-walkthrough\n",
        "T2T_USR_DIR=\"$BASEDIR/t2t-usr\"\n",
        "PYTHONPATH=$T2T_USR_DIR:$PYTHONPATH\n",
        "echo $PYTHONPATH\n",
        "python /content/sgnmt/decode.py --config_file=decode_adapted_winomt/decode.ini --range=1:1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMR3tcP9mNSZ",
        "colab_type": "text"
      },
      "source": [
        "Once decoding is finished and `$decode_dir/out.text` exists, we  convert the output to plaintext for evaluation and evalute using WinoMT as before (if the entire test set has not been decoded, WinoMT will throw an error about  \"not enough values to unpack\".)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwxmDNA0Bu_I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bash  \n",
        "BASEDIR=/content/drive/My\\ Drive/gender-debias-walkthrough\n",
        "CONVERT_SCRIPT=\"$BASEDIR\"/scripts/convert_id_to_detok.sh\n",
        "WINOMT_PREP_SCRIPT=\"$BASEDIR\"/scripts/prep_for_winomt_eval.sh\n",
        "lang=de\n",
        "winomt_logs=winomtout\n",
        "\n",
        "bash \"$CONVERT_SCRIPT\" $lang detok < decode_adapted_winomt/output.ids 1> decode_adapted_winomt/output.detok\n",
        "bash \"$WINOMT_PREP_SCRIPT\" decode_adapted_winomt/output.detok $lang /content/mt_gender\n",
        "cd mt_gender/src\n",
        "export FAST_ALIGN_BASE=/content/fast_align/\n",
        "bash ../scripts/evaluate_all_languages.sh ../data/aggregates/en.txt $winomt_logs\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqtmGW2Qm4UQ",
        "colab_type": "text"
      },
      "source": [
        "# The catastrophic forgetting problem\n",
        "\n",
        "Unfortuantely, a known downside of domain adaptation for neural models is that the models tend to experience `catastrophic forgetting' of anything they've seen previously.\n",
        "\n",
        "In a way, we're relying on this effect! We want the model to quickly forget the gender bias it has seen during baseline training. \n",
        "\n",
        "But the NMT model has also seen good, fluent examples of translation which let it translate well in general. We don't want forgetting to impact this ability.\n",
        "\n",
        "We can assess forgetting by measuring translation performance in [BLEU points](https://en.wikipedia.org/wiki/BLEU) on standard translation test sets. \n",
        "\n",
        "First we have to translate the test set just as we did for WinoMT."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BF8GqZsVBMQ9",
        "colab_type": "text"
      },
      "source": [
        "To decode the general test set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URRILHcvl9eT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bash\n",
        "\n",
        "BASEDIR=/content/drive/My\\ Drive/gender-debias-walkthrough\n",
        "SRC_VOCAB_SIZE=$(wc -l < \"$BASEDIR/data_ende/handcrafted_ende/vocab.src\")\n",
        "TRG_VOCAB_SIZE=$(wc -l < \"$BASEDIR/data_ende/handcrafted_ende/vocab.trg\")\n",
        "T2T_USR_DIR=\"$BASEDIR/t2t-usr\"\n",
        "SRC_TEST=\"$BASEDIR/data_ende/test.ende.src.ids\"\n",
        "model_dir=model/ft/ende/\n",
        "decode_dir=decode_adapted\n",
        "mkdir -p $decode_dir\n",
        "\n",
        "config_file=$decode_dir/decode.ini\n",
        "echo \"verbosity: debug\n",
        "predictors: t2t\n",
        "src_test: \"$SRC_TEST\"\n",
        "decoder: beam\n",
        "beam: 4\n",
        "t2t_model: transformer\n",
        "t2t_hparams_set: transformer_base\n",
        "t2t_problem: translate_generic_existing_vocab\n",
        "pred_src_vocab_size: \"$SRC_VOCAB_SIZE\"\n",
        "pred_trg_vocab_size: \"$TRG_VOCAB_SIZE\"\n",
        "indexing_scheme: t2t\n",
        "t2t_usr_dir: \"$T2T_USR_DIR\"\n",
        "t2t_unk_id: 3\n",
        "output_path: \"$decode_dir\"/output.ids\n",
        "t2t_checkpoint_dir: \"$model_dir\"\n",
        "outputs: text\" > $config_file"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9ibLLlzCYA6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bash \n",
        "source activate sgnmt_env\n",
        "BASEDIR=/content/drive/My\\ Drive/gender-debias-walkthrough\n",
        "T2T_USR_DIR=\"$BASEDIR/t2t-usr\"\n",
        "PYTHONPATH=$T2T_USR_DIR:$PYTHONPATH\n",
        "python /content/sgnmt/decode.py --config_file=decode_adapted/decode.ini --range=1:1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTx2Q6SvqtnA",
        "colab_type": "text"
      },
      "source": [
        "When the test set is translated we evaluate it automatically against reference translations using a tool called sacreBLEU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbNgPcbamsDQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bash  \n",
        "BASEDIR=/content/drive/My\\ Drive/gender-debias-walkthrough\n",
        "REF=\"$BASEDIR\"/data_ende/test.ende.trg.detok \n",
        "CONVERT_SCRIPT=\"$BASEDIR\"/scripts/convert_id_to_detok.sh\n",
        "lang=de\n",
        "bash \"$CONVERT_SCRIPT\" $lang detok < decode_adapted/output.ids 1> decode_adapted/output.detok\n",
        "## If you have decoded the entire test set, run sacrebleu against the entire reference\n",
        "head -1 \"$REF\" > ref.line1\n",
        "sacrebleu ref.line1 < decode_adapted/output.detok\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d30HyBsQplDG",
        "colab_type": "text"
      },
      "source": [
        "If you decode and evaluate the test set with both the baseline and adapted models, you'll see that there's a noticeable drop in translation performance as measured by BLEU.\n",
        "\n",
        "One way around this is to constrain the adapted model to only produce gender-inflected versions of the baseline model's translation. This is a form of lattice rescoring. Essentially, it should ensure the adapted model can only deviate from the baseline translation to produce different gender inflections."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ozseu93sU-4",
        "colab_type": "text"
      },
      "source": [
        "# Lattice rescoring: \"Correcting\" gender bias in translations\n",
        "\n",
        "Our alternative solution doesn't try to avoid forgetting when adapting the model. Instead, we fully embrace the forgetting, which gives good improvements on gender debiasing. But during decoding, we try to keep the translation close to a previously-generated translation (which may contain gender-bias-related mistakes.)\n",
        "\n",
        "In fact, we limit the translation model to produce only gender-inflected versions of the original translation.\n",
        "\n",
        "For example our baseline model translates the English sentence in WinoMT:\n",
        "\n",
        "*The cleaner hates **the developer** because **she** always leaves the room dirty.*\n",
        "\n",
        "to the German\n",
        "\n",
        "*Der Reiniger haßt **den Entwickler**, weil **er** den Raum immer schmutzig l&auml;sst.*\n",
        "\n",
        "Both bolded parts in the German sentence are (incorrectly) masculine forms. \n",
        "\n",
        "But we can use automatic tools with giant vocabulary lists to find that in German, *Entwickler* has an alternate inflected form *Entwicklerin*, and the masculine pronoun *er* has an equivalent feminine inflection *sie*. \n",
        "\n",
        "Of course, we don't know which words in the translation might be wrongly inflected, so we go through this process and find alternate inflections for every word in every translated sentence. We represent this series of alternate inflections as a lattice using OpenFST tools.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tc05JFO6tYCj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bash  \n",
        "BASEDIR=/content/drive/My\\ Drive/gender-debias-walkthrough\n",
        "INFLECTED_VOCAB_MAPPINGS=\"$BASEDIR/data_ende/inflections-de\"\n",
        "FST_SYMS=\"$BASEDIR/data_shared/fst_syms\"\n",
        "APPLY_BPE_SCRIPT=\"$BASEDIR/scripts/apply_bpe.sh\"\n",
        "SENTENCE_TO_FST=\"$BASEDIR/scripts/sentence_to_fst.py\"\n",
        "INFLECTIONS_TO_FST=\"$BASEDIR/scripts/multi_options_to_fst.py\"\n",
        "CONVERT_SCRIPT=\"$BASEDIR\"/scripts/convert_id_to_detok.sh\n",
        "lang=de\n",
        "bash \"$CONVERT_SCRIPT\" $lang < decode_adapted_winomt/output.ids 1> decode_adapted_winomt/output.tok\n",
        "\n",
        "plaintext_hyps=decode_adapted_winomt/output.tok\n",
        "sentence_count=$(wc -l < \"$plaintext_hyps\")\n",
        "\n",
        "lattice_dir=winomt_lattices.de\n",
        "mkdir -p $lattice_dir\n",
        "for i in $(seq $sentence_count); do\n",
        "    tmpfsttext=$lattice_dir/$i.tmp.fst\n",
        "    awk \"NR==$i\" $plaintext_hyps | bash \"$APPLY_BPE_SCRIPT\" $lang | python \"$SENTENCE_TO_FST\" \\\n",
        "    | fstcompile --isymbols=\"$FST_SYMS\"  --osymbols=\"$FST_SYMS\" > $tmpfsttext;\n",
        "    awk \"NR==$i\" $plaintext_hyps |  tr ' ' '\\n' | sed 's/^/ /g' | sort -u \\\n",
        "    | fgrep -f - \"$INFLECTED_VOCAB_MAPPINGS\" | bash \"$APPLY_BPE_SCRIPT\" $lang | sed 's/ 1 / | /g' \\\n",
        "    | python \"$INFLECTIONS_TO_FST\" | fstcompile --isymbols=\"$FST_SYMS\" --osymbols=\"$FST_SYMS\" | fstcompose $tmpfsttext - |  fstminimize --allow_nondet=true \\\n",
        "    > $lattice_dir/$i.fst\n",
        "    rm $tmpfsttext\n",
        "done\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsEj3iVTDBQY",
        "colab_type": "text"
      },
      "source": [
        "When creating a config file for SGNMT decoding, we can now instruct the decoder to constrain search to the provided lattices using the \"predictors\" and \"fst_path\" options."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZhQcE0VzxSX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bash\n",
        "\n",
        "BASEDIR=/content/drive/My\\ Drive/gender-debias-walkthrough\n",
        "SRC_VOCAB_SIZE=$(wc -l < \"$BASEDIR/data_ende/handcrafted_ende/vocab.src\")\n",
        "TRG_VOCAB_SIZE=$(wc -l < \"$BASEDIR/data_ende/handcrafted_ende/vocab.trg\")\n",
        "T2T_USR_DIR=\"$BASEDIR/t2t-usr\"\n",
        "SRC_TEST=\"$BASEDIR/data_ende/winomt.ende.src.ids\"\n",
        "lattice_dir=winomt_lattices.de\n",
        "model_dir=model/ft/ende/\n",
        "decode_dir=decode_adapted_winomt.lattices\n",
        "mkdir -p $decode_dir\n",
        "\n",
        "config_file=$decode_dir/decode.ini\n",
        "echo \"verbosity: debug\n",
        "predictors: t2t,nfst\n",
        "src_test: \"$SRC_TEST\"\n",
        "decoder: beam\n",
        "beam: 4\n",
        "t2t_model: transformer\n",
        "t2t_hparams_set: transformer_base\n",
        "t2t_problem: translate_generic_existing_vocab\n",
        "pred_src_vocab_size: \"$SRC_VOCAB_SIZE\"\n",
        "pred_trg_vocab_size: \"$TRG_VOCAB_SIZE\"\n",
        "indexing_scheme: t2t\n",
        "t2t_usr_dir: \"$T2T_USR_DIR\"\n",
        "t2t_unk_id: 3\n",
        "output_path: \"$decode_dir\"/output.ids\n",
        "fst_path: \"$lattice_dir/%d.fst\"\n",
        "t2t_checkpoint_dir: \"$model_dir\"\n",
        "outputs: text\" > $config_file\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a5I6w-9x-h5",
        "colab_type": "text"
      },
      "source": [
        "We can now decode with the debiased model, constraining it to produce only words in the lattice. Effectively it can only produce alternately-inflected forms of the original translation. Because the original, fluent baseline model gets control of the words in the translation, and the debiased model only changes inflections, we would hope that translation quality doesn't change much."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9aMuHXGBC70n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bash \n",
        "source activate sgnmt_env\n",
        "\n",
        "FSTPATH=/usr/local/lib/python3.6/dist-packages/openfst_python\n",
        "\n",
        "BASEDIR=/content/drive/My\\ Drive/gender-debias-walkthrough\n",
        "T2T_USR_DIR=\"$BASEDIR/t2t-usr\"\n",
        "PYTHONPATH=\"$FSTPATH:$T2T_USR_DIR:$PYTHONPATH\"\n",
        "\n",
        "python /content/sgnmt/decode.py --config_file=decode_adapted_winomt.lattices/decode.ini --range=1:1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQirsZe3z5wM",
        "colab_type": "text"
      },
      "source": [
        "If we do this to the general test set, we shouldn't see much change in BLEU. If we do this to the WinoMT test set, we see almost as much reduction in gender bias as for the adapted model.\n",
        "\n",
        "One particularly good feature of lattice rescoring is that we don't actually need the baseline model at all - just its translation hypotheses. We can therefore also apply this lattice-rescoring method to the output of commercial translation systems as collected by Stanovsky et al, and reduce gender bias in those as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuoBpN9oBgnY",
        "colab_type": "text"
      },
      "source": [
        "# Elastic Weight Consolidation: balancing gender debiasing and translation fluency\n",
        "\n",
        "An alternative way to deal with the catastrophic forgetting problem is [Elastic Weight Consolidation (EWC)](https://arxiv.org/abs/1612.00796). \n",
        "\n",
        "The general idea of EWC is to estimate how important different parameters in the neural network are to a task (general translation ability.) \n",
        "\n",
        "Then, when adapting the model to a new task (gender debiasing) we just apply a larger penalty to changing a parameter if the parameter was important for the previous task.\n",
        "\n",
        "EWC is not implemented in main Tensor2Tensor, so we first set up an environment for an older forked version with EWC implemented."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lm8pyEU97dC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bash\n",
        "# make local copy of env file so conda doesn't attempt to write to drive\n",
        "cp \"/content/drive/My Drive/gender-debias-walkthrough/data_shared/env.yml\" /content/env.yml\n",
        "conda create -y -n tf_env_ewc pip \n",
        "conda activate tf_env_ewc\n",
        "conda env update --file /content/env.yml   -n tf_env_ewc\n",
        "git clone --single-branch -b dsaunders_v1.4.3_modified-ewc  https://github.com/DCSaunders/tensor2tensor.git\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ow0Vp49G-1Ah",
        "colab_type": "text"
      },
      "source": [
        "Using this version of T2T we run adaptation to the same handcrafted set, now applying a loss penalty to the pre-saved EWC fisher variables:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-T5tvaK8r2RY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bash \n",
        "tmppythonpath=$PYTHONPATH\n",
        "source activate tf_cpu\n",
        "BASEDIR='/content/drive/My Drive/gender-debias-walkthrough/'\n",
        "SRC_DIR=\"$BASEDIR/baseline_ende\"\n",
        "EWC_VARS=\"$BASEDIR/baseline_ende/ewc_vars\"\n",
        "\n",
        "DATA_DIR=\"$BASEDIR/data_ende/handcrafted_ende\"\n",
        "\n",
        "BASE_STEPS=300000\n",
        "adapt_steps=4\n",
        "TRAIN_STEPS==$(( $BASE_STEPS + $adapt_steps ))\n",
        "batch_size=4096\n",
        "ewc_loss_weight=10000000\n",
        "T2T=\"/content/tensor2tensor/\"\n",
        "export PYTHONPATH=\"$T2T:$PYTHONPATH\" \n",
        "\n",
        "model_dir=model/ft/ende_ewc/\n",
        "mkdir -p $model_dir\n",
        "\n",
        "# make a local copy of the model\n",
        "cp \"$SRC_DIR\"/model* $model_dir/\n",
        "cp \"$SRC_DIR\"/checkpoint $model_dir/\n",
        "ln -s \"$EWC_VARS\" \"$model_dir/ewc_vars\"\n",
        "\n",
        "python $T2T/tensor2tensor/bin/t2t_trainer.py \\\n",
        " --data_dir=\"$DATA_DIR\" \\\n",
        " --problems=translate_generic_existing_vocab --hparams_set=transformer_base \\\n",
        " --output_dir=$model_dir --model=transformer \\\n",
        " --schedule=train --train_steps=300004 --keep_checkpoint_max=1  \\\n",
        "--hparams=\"batch_size=$batch_size,ewc_load_vars=True,ewc_loss_weight=$ewc_loss_weight\"\n",
        "$PYTHONPATH=$tmppythonpath\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Duyx2vLdsBS3",
        "colab_type": "text"
      },
      "source": [
        "Although EWC reduces catastrophic forgetting, it also results in some trade-off between reduced gender bias and general translation ability. It also cannot be applied to black-box translations like lattice rescoring can. However, it does involve only a single model and decoding pass."
      ]
    }
  ]
}