{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gender-debias.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "25_Rkxp1vzqy",
        "D32bzB82REm9",
        "SqtmGW2Qm4UQ",
        "3ozseu93sU-4"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DCSaunders/gender-debias/blob/master/gender_debias.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UN_D-dlktVU"
      },
      "source": [
        "# Reducing gender bias in neural machine translation as a domain adaptation problem\n",
        "This notebook walks through the paper [Reducing Gender Bias in Neural Machine Translation as a Domain Adaptation Problem](https://arxiv.org/abs/2004.04498):\n",
        "\n",
        "\n",
        "\n",
        "*   What the gender bias problem is in Neural Machine Translation (NMT), and how we can measure it.\n",
        "*   How to debias an NMT model by *domain adaptation*: fine-tuning on a new, small, debiased dataset\n",
        "*   How to avoid *catastrophic forgetting*: debiasing the model while keeping its general translation ability\n",
        "*   How to use the debiased model for *lattice rescoring*: \"correcting\" words with the wrong gender inflection in machine translations.\n",
        "\n",
        "This notebook walks through steps to reproduce our primary experiments, including tool installation, our baseline models and adaptation data.\n",
        "\n",
        "**IMPORTANT: the following notebook has been tested with python 3.6 and tensorflow 1.5.0 in order to use tensor2tensor 1.4.3. However, default colab installations change. We recommend running these steps on a local machine if possible for full control over software versions.**\n",
        "\n",
        "<!-- \n",
        "Otherwise, to reproduce locally:\n",
        "\n",
        "* Download the desired language pair files from the Google Drive https://drive.google.com/drive/folders/1XaWbXQQ8icZZE__fPBOj4dC24oiA3Qtv?usp=sharing  (For example, if you just want to reproduce our English-German experiments, you can ignore the folders ending with \"enes\" or \"enhe\" but should take everything else). These folders include our BPE vocabulary mappings and scripts to convert between plaintext and id-maps.\n",
        "* Download Tensor2Tensor version 1.4.3 or more recent if you wish to use our baseline models for fine-tuning. The main repo is https://github.com/tensorflow/tensor2tensor - if you want to use our EWC implementation, check out https://github.com/DCSaunders/tensor2tensor/tree/ewc.v1.5\n",
        "* Download OpenFST http://www.openfst.org/twiki/bin/view/FST/WebHome if you wish to generate translation lattices for lattice rescoring\n",
        "* Download SGNMT https://github.com/ucam-smt/sgnmt if you wish to use our setup for decoding, including implementations for rescoring using OpenFST lattices\n",
        "* Download sacrebleu https://github.com/mjpost/sacreBLEU/ for reproducible BLEU score evaluation\n",
        "* Download WinoMT https://github.com/gabrielStanovsky/mt_gender for gender bias challenge set evaluation -->\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkhIGZfb7KQ4"
      },
      "source": [
        "## First set up all tools used in this notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dw0oSwknN1Mg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24f131be-81cf-4e87-ffa7-7589a69647e8"
      },
      "source": [
        "# install miniconda\n",
        "%env PYTHONPATH=\n",
        "!wget -c https://repo.anaconda.com/miniconda/Miniconda3-4.5.4-Linux-x86_64.sh\n",
        "!chmod +x Miniconda3-4.5.4-Linux-x86_64.sh\n",
        "!bash ./Miniconda3-4.5.4-Linux-x86_64.sh -b -f -p /usr/local\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "env: PYTHONPATH=\n",
            "--2021-03-18 15:50:30--  https://repo.anaconda.com/miniconda/Miniconda3-4.5.4-Linux-x86_64.sh\n",
            "Resolving repo.anaconda.com (repo.anaconda.com)... 104.16.130.3, 104.16.131.3, 2606:4700::6810:8303, ...\n",
            "Connecting to repo.anaconda.com (repo.anaconda.com)|104.16.130.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable\n",
            "\n",
            "    The file is already fully retrieved; nothing to do.\n",
            "\n",
            "PREFIX=/usr/local\n",
            "installing: python-3.6.5-hc3d631a_2 ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocrI1nZ37JUD"
      },
      "source": [
        "# Install tools tensor2tensor dependency.   \n",
        "# Restart runtime if prompted\n",
        "!conda create -y -n tf_env_main pip python=3.6\n",
        "! source activate tf_env_main\n",
        "!pip install cloudpickle==1.2.0\n",
        "!pip install tensorflow==1.5.0\n",
        "!pip install -q -U tensor2tensor==1.4.3\n",
        "# restart if prompted \n",
        "import sys\n",
        "if 'google.colab' in sys.modules: # Colab-only TensorFlow version selector\n",
        "  %tensorflow_version 1.15.2\n",
        "import tensorflow as tf\n",
        "import tensor2tensor\n",
        "!pip install -q openfst-python\n",
        "! pip install sacrebleu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcUpcxSk9QEP"
      },
      "source": [
        "Set up environment for SGNMT decoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyKmqiq_9hjB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c98ec532-f095-4dce-a48d-8ab58324b752"
      },
      "source": [
        "%%bash\n",
        "conda install -y -q -c conda-forge openfst\n",
        "conda create -y -n sgnmt_env pip python=3.6\n",
        "conda install --name sgnmt_env -y -q -c conda-forge openfst\n",
        "source activate sgnmt_env\n",
        "git clone https://github.com/ucam-smt/sgnmt.git\n",
        "pip install -q openfst-python\n",
        "conda install --name sgnmt_env -y -q -f numpy pyyaml  scipy\"==1.0.0\"  \n",
        "conda install --name sgnmt_env -y -q -c conda-forge openfst\n",
        "\n",
        "conda install tensorflow==1.5.0\n",
        "conda install -q -U tensor2tensor==1.4.3\n",
        "\n",
        "FSTPATH=/usr/local/lib/python3.6/dist-packages/openfst_python\n",
        "PYTHONPATH=$FSTPATH:$PYTHONPATH\n",
        "## verify\n",
        "echo $PYTHONPATH\n",
        "python sgnmt/decode.py --run_diagnostics\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Process is terminated.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjWjXgSW8FKe"
      },
      "source": [
        "Check that installations behave correctly inside python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccNsKtir9HqP"
      },
      "source": [
        "Set up gender bias evaluation requirements\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nf2bWQOx9HO5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a78403d7-78cc-43a0-c98b-863ae19c9640"
      },
      "source": [
        "%%bash\n",
        "echo \"git clone fast_align\"\n",
        "git clone https://github.com/clab/fast_align.git\n",
        "cd fast_align\n",
        "mkdir -p build\n",
        "cd build\n",
        "echo \"cmake\"\n",
        "cmake ..\n",
        "echo \"make\"\n",
        "make\n",
        "echo \"test fast align\"\n",
        "/content/fast_align/build/fast_align\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "git clone fast_align\n",
            "cmake\n",
            "-- Could NOT find SparseHash (missing: SPARSEHASH_INCLUDE_DIR) \n",
            "-- Configuring done\n",
            "-- Generating done\n",
            "-- Build files have been written to: /content/fast_align/build\n",
            "make\n",
            "[ 50%] Built target atools\n",
            "[100%] Built target fast_align\n",
            "test fast align\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'fast_align' already exists and is not an empty directory.\n",
            "Usage: /content/fast_align/build/fast_align -i file.fr-en\n",
            " Standard options ([USE] = strongly recommended):\n",
            "  -i: [REQ] Input parallel corpus\n",
            "  -v: [USE] Use Dirichlet prior on lexical translation distributions\n",
            "  -d: [USE] Favor alignment points close to the monotonic diagonoal\n",
            "  -o: [USE] Optimize how close to the diagonal alignment points should be\n",
            "  -r: Run alignment in reverse (condition on target and predict source)\n",
            "  -c: Output conditional probability table\n",
            " Advanced options:\n",
            "  -I: number of iterations in EM training (default = 5)\n",
            "  -q: p_null parameter (default = 0.08)\n",
            "  -N: No null word\n",
            "  -a: alpha parameter for optional Dirichlet prior (default = 0.01)\n",
            "  -T: starting lambda for diagonal distance parameter (default = 4)\n",
            "  -s: print alignment scores (alignment ||| score, disabled by default)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WesY74dgDeoH"
      },
      "source": [
        "%%bash\n",
        "git clone https://github.com/gabrielStanovsky/mt_gender.git\n",
        "cd mt_gender\n",
        "FAST_ALIGN=/content/fast_align/\n",
        "./install.sh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMqGGvDA7wtk"
      },
      "source": [
        "The files and scripts needed to run these GDB are available from a shared Google drive which you will need to link to your own Google drive, as follows:\n",
        "\n",
        "1. In a seperate browser window, go to  https://drive.google.com/drive/folders/1XaWbXQQ8icZZE__fPBOj4dC24oiA3Qtv?usp=sharing \n",
        "1. Select `gender-debias-walkthrough`\n",
        "1. Click on `Add shortcut to Drive`\n",
        "\n",
        "You should now see a link to the folder `gender-debias-walkthrough` in `My Drive`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOo_gZoR7mbC"
      },
      "source": [
        "Mount your google drive so this folder is visible in this notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHnJyCKA7q1u"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "! ls -d /content/drive/My\\ Drive\n",
        "! ls /content/drive/My\\ Drive/gender-debias-walkthrough"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25_Rkxp1vzqy"
      },
      "source": [
        "# The gender bias problem\n",
        "\n",
        "Let's say we want to train a neural network to translate from English to German. We can do this by showing the model lots of examples of real English sentences and their German translations.\n",
        "\n",
        "But we have to get those example translations from somewhere. News reports, political speeches, Wikipedia and TED talks are common sources. Society being what it is, these tend to have more examples of men than of women.\n",
        "\n",
        "In fact we can count the number of sentences in the English side of the dataset which contain masculine words like *he, him, his, man, men*, etc, and do the same for the equivalent feminine words. We find there are about twice as many masculine sentences.\n",
        "\n",
        "German, like many languages, has grammatical gender. *The doctor* should be translated into German as *Der Arzt* if the doctor is male,  *Die &Auml;rztin* if the doctor is female. But because of data bias, machine translation models often get this wrong.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMtV5_pcxBai"
      },
      "source": [
        "# Can we measure gender bias in translation?\n",
        "\n",
        "\n",
        "Yes! [Stanovsky et al (2019)](https://www.aclweb.org/anthology/P19-1164.pdf)  recently developed the [WinoMT framework](https://github.com/gabrielStanovsky/mt_gender) for this purpose. WinoMT comes with a set of 3888 sentences, for example:\n",
        "\n",
        "*The **physician** told the nurse that **he** had been busy.*\n",
        "\n",
        "*The **physician** told the nurse that **she** had been busy.*\n",
        "\n",
        "\n",
        "\n",
        "*   Each sentence has a primary entity -- *the physician* in these examples.\n",
        "*  Each primary entity is coreferent with a pronoun -- *he* in the first example, *she* in the second.\n",
        "* WinoMT is gender-balanced: there are the same total number of male and female physicians (and every other kind of primary entity)\n",
        "\n",
        "If you translate these 3888 sentences from English to one of 8 supported languages with grammatical gender, WinoMT provides an automatic evaluation framework to see how much gender bias is in those translations:\n",
        "\n",
        "*   It automatically aligns your translations with the English input to find your translated version of the primary entity\n",
        "*   Using morphological analysis tools, WinoMT extracts the translated primary entity's grammatical gender\n",
        "*   The objective is to achieve the correct grammatical gender for each sentence. WinoMT reports this **accuracy** as a percentage over all sentences.\n",
        "*   Because WinoMT has the same number of male and female sentences, it can also report the difference in translation quality between them as **∆G**, the difference in [F1 score](https://en.wikipedia.org/wiki/F1_score) between sentences with male and female entities. If ∆G is above 0, it gets the correct gender for male entities more frequently than for female entities.\n",
        "\n",
        "Stanovsky et al report results on several commercial translation systems, like Google and Bing translate. We can also use our own baseline model to translate the WinoMT set and evaluate it to see the same problem.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F245X0snksh6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58b23402-4f3f-4b01-8168-4f728691df2e"
      },
      "source": [
        "%%bash\n",
        "BASEDIR='/content/drive/My Drive/gender-debias-walkthrough/'\n",
        "WINOMT_PREP_SCRIPT=\"$BASEDIR/scripts/prep_for_winomt_eval.sh\"\n",
        "BASELINE_HYP=\"$BASEDIR/data_ende/winomt.ende.baseline-hyp.detok\"\n",
        "lang=de \n",
        "winomt_logs=/content/winomtout\n",
        "mkdir -p $winomt_logs\n",
        "bash \"$WINOMT_PREP_SCRIPT\" \"$BASELINE_HYP\" $lang /content/mt_gender\n",
        "cd /content/mt_gender/src\n",
        "export FAST_ALIGN_BASE=/content/fast_align/\n",
        "bash ../scripts/evaluate_all_languages.sh ../data/aggregates/en.txt $winomt_logs/ &>$winomt_logs/baseline\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Process is interrupted.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pw9nx4LLieYh"
      },
      "source": [
        "We can now look at the log file to get the WinoMT accuracy and ∆G score for the scored model, as well as more detailed information about the proportion of translations were male vs female.\n",
        "\n",
        "This baseline English-German model gets 60.1% WinoMT accuracy.\n",
        "\n",
        "The model's ∆G score is 69.1 - 50.5 = 18.6.\n",
        "\n",
        "These scores correspond to the baseline en-de results in Tables 2, 3 and 4 of [the paper](https://arxiv.org/pdf/2004.04498.pdf)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0aLdYgGieEP"
      },
      "source": [
        "%%bash\n",
        "winomt_logs=/content/winomtout\n",
        "grep acc $winomt_logs/baseline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D32bzB82REm9"
      },
      "source": [
        "# What can we do about gender bias in machine translation?\n",
        "There's been increasing interest in reducing gender bias in language processing tasks in the last year or two. Proposed solutions include trying to [debias parts of the model before or during training](https://www.aclweb.org/anthology/W19-3821.pdf), or [producing multiple differently gendered translations in ambiguous cases](https://www.blog.google/products/translate/reducing-gender-bias-google-translate/). \n",
        "\n",
        "Our approach is based on domain adaptation: we want to adapt the model to a domain which has no gender bias, encouraging it to forget about any previously seen data bias."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0V8CGauUXdEx"
      },
      "source": [
        "# Adapting a model to balanced data\n",
        "\n",
        "We can do this very quickly with a tiny dataset which we create in English and translate into target languages like German. The dataset contains sentences pairs like:\n",
        "\n",
        "\n",
        "*The doctor finished his work. | Der Arzt beendete seine Arbeit.*\n",
        "\n",
        "*The doctor finished her work. | Die Ärztin beendete ihre Arbeit.*\n",
        "\n",
        "And so on for 194 professions, for a total of just 388 sentences.\n",
        "\n",
        "Since the adaptation dataset is very small, we only iterate over it a few times before convergence. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vo7KUrn_Wu88",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d222bc9c-3b7c-452c-ce75-b16badb46822"
      },
      "source": [
        "%%bash\n",
        "source activate tf_env_main\n",
        "\n",
        "BASEDIR='/content/drive/My Drive/gender-debias-walkthrough/'\n",
        "SRC_DIR=\"$BASEDIR/baseline_ende\"\n",
        "T2T_USR_DIR=\"$BASEDIR/t2t-usr\"\n",
        "#T2T=\"/tensorflow-1.15.2/python3.7/tensor2tensor/bin/\"\n",
        "#TF=\"/tensorflow-1.15.2/python3.7/bin\"\n",
        "T2T=\"/usr/local/lib/python3.6/site-packages/tensor2tensor/\"\n",
        "TF=\"/usr/local/lib/python3.6/site-packages/tensorflow/\"\n",
        "DATA_DIR=\"$BASEDIR/data_ende/handcrafted_ende\"\n",
        "PYTHONPATH=/usr/local/lib/python3.6/site-packages:$T2T_USR_DIR:$PYTHONPATH\n",
        "\n",
        "BASE_STEPS=300000\n",
        "adapt_steps=4\n",
        "TRAIN_STEPS==$(( $BASE_STEPS + $adapt_steps ))\n",
        "batch_size=4096\n",
        "\n",
        "model_dir=model/ft/ende/\n",
        "mkdir -p $model_dir\n",
        "\n",
        "# make a local copy of T2T_USR_DIR in t2t-usr\n",
        "cp \"$SRC_DIR\"/model* $model_dir/\n",
        "cp \"$SRC_DIR\"/checkpoint $model_dir/\n",
        "\n",
        "#%env PYTHONPATH=\"$T2T:$T2T_USR_DIR:$TF:$PYTHONPATH\" \n",
        "#%env PATH=\"$T2T:$T2T_USR_DIR:$TF:$PATH\" \n",
        "\n",
        "python /usr/local/lib/python3.6/site-packages/tensor2tensor/bin/t2t_trainer.py \\\n",
        " --data_dir=\"$DATA_DIR\" \\\n",
        " --problems=translate_generic_existing_vocab --hparams_set=transformer_base \\\n",
        " --t2t_usr_dir=\"$T2T_USR_DIR\" --output_dir=$model_dir --model=transformer \\\n",
        " --schedule=train --train_steps=300004 --keep_checkpoint_max=1  \\\n",
        " --hparams=\"batch_size=$batch_size\" \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:493: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:494: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:495: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:496: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:497: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:502: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "INFO:tensorflow:Importing user module t2t-usr from path /content/drive/My Drive/gender-debias-walkthrough\n",
            "INFO:tensorflow:schedule=train\n",
            "INFO:tensorflow:worker_gpu=1\n",
            "INFO:tensorflow:sync=False\n",
            "INFO:tensorflow:datashard_devices: ['/job:localhost']\n",
            "INFO:tensorflow:caching_devices: None\n",
            "INFO:tensorflow:ps_devices: ['gpu:0']\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f86a0813278>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': 1234, '_save_summary_steps': 100, '_save_checkpoints_secs': None, '_log_step_count_steps': 100, '_session_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 0.95\n",
            "}\n",
            "allow_soft_placement: true\n",
            "graph_options {\n",
            "  optimizer_options {\n",
            "  }\n",
            "}\n",
            ", '_save_checkpoints_steps': 1000, '_keep_checkpoint_max': 1, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': 'model/ft/ende/', 'use_tpu': False, 't2t_device_info': {'num_async_replicas': 1}, 'data_parallelism': <tensor2tensor.utils.expert_utils.Parallelism object at 0x7f86bce96ef0>}\n",
            "WARNING:tensorflow:Estimator's model_fn (<function T2TModel.make_estimator_model_fn.<locals>.wrapping_model_fn at 0x7f86a9668c80>) includes params argument, but params are not passed to Estimator.\n",
            "INFO:tensorflow:Reading data files from /content/drive/My Drive/gender-debias-walkthrough//data_ende/handcrafted_ende/translate_generic_existing_vocab-train*\n",
            "INFO:tensorflow:partition: 0 num_data_files: 1\n",
            "INFO:tensorflow:Setting T2TModel mode to 'train'\n",
            "INFO:tensorflow:Using variable initializer: uniform_unit_scaling\n",
            "INFO:tensorflow:Transforming feature 'inputs' with symbol_modality_35627_512.bottom\n",
            "INFO:tensorflow:Transforming 'targets' with symbol_modality_35627_512.targets_bottom\n",
            "INFO:tensorflow:Building model body\n",
            "INFO:tensorflow:Transforming body output with symbol_modality_35627_512.top\n",
            "INFO:tensorflow:Base learning rate: 0.200000\n",
            "INFO:tensorflow:Applying exp learning rate warmup for 8000 steps\n",
            "INFO:tensorflow:Applying learning rate decay: noam.\n",
            "INFO:tensorflow:Trainable Variables Total size: 62361088\n",
            "INFO:tensorflow:Using optimizer Adam\n",
            "/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:97: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "2021-03-18 14:59:40.966121: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
            "INFO:tensorflow:Restoring parameters from model/ft/ende/model.ckpt-300000\n",
            "INFO:tensorflow:Saving checkpoints for 300001 into model/ft/ende/model.ckpt.\n",
            "INFO:tensorflow:loss = 1.4521875, step = 300001\n",
            "INFO:tensorflow:Saving checkpoints for 300004 into model/ft/ende/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.66352576.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xP_Ibi0YKrQ"
      },
      "source": [
        "# Inference with the adapted model\n",
        "Having fine-tuned the model on the balanced dataset, we can use it to translate the WinoMT test set again, using SGNMT to handle beam search decoding. We first create a config file which specifies what we want to decode and how we want to decode.\n",
        "\n",
        "To decode the WinoMT test set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEYS8KjGBLPb"
      },
      "source": [
        "%%bash\n",
        "\n",
        "BASEDIR=/content/drive/My\\ Drive/gender-debias-walkthrough\n",
        "SRC_VOCAB_SIZE=$(wc -l < \"$BASEDIR/data_ende/handcrafted_ende/vocab.src\")\n",
        "TRG_VOCAB_SIZE=$(wc -l < \"$BASEDIR/data_ende/handcrafted_ende/vocab.trg\")\n",
        "T2T_USR_DIR=\"$BASEDIR/t2t-usr\"\n",
        "SRC_TEST=\"$BASEDIR/data_ende/winomt.ende.src.ids\"\n",
        "model_dir=model/ft/ende/\n",
        "decode_dir=decode_adapted_winomt\n",
        "mkdir -p $decode_dir\n",
        "\n",
        "config_file=$decode_dir/decode.ini\n",
        "echo \"verbosity: debug\n",
        "predictors: t2t\n",
        "src_test: \"$SRC_TEST\"\n",
        "decoder: beam\n",
        "beam: 4\n",
        "t2t_model: transformer\n",
        "t2t_hparams_set: transformer_base\n",
        "t2t_problem: translate_generic_existing_vocab\n",
        "pred_src_vocab_size: \"$SRC_VOCAB_SIZE\"\n",
        "pred_trg_vocab_size: \"$TRG_VOCAB_SIZE\"\n",
        "indexing_scheme: t2t\n",
        "t2t_usr_dir: \"$T2T_USR_DIR\"\n",
        "t2t_unk_id: 3\n",
        "output_path: \"$decode_dir\"/output.ids\n",
        "t2t_checkpoint_dir: \"$model_dir\"\n",
        "outputs: text\" > $config_file\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHPOUYD1At-o"
      },
      "source": [
        "Then we activate the decoding environment and run decoding.\n",
        "\n",
        "Warning: the  ```--range=1:1``` option decodes only the first sentence. Removing the range option will default to decoding the entire test set, which could take a long time.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKH0DmKxAtzH"
      },
      "source": [
        "%%bash \n",
        "source activate sgnmt_env\n",
        "BASEDIR=/content/drive/My\\ Drive/gender-debias-walkthrough\n",
        "T2T_USR_DIR=\"$BASEDIR/t2t-usr\"\n",
        "PYTHONPATH=$T2T_USR_DIR:$PYTHONPATH\n",
        "echo $PYTHONPATH\n",
        "python /content/sgnmt/decode.py --config_file=decode_adapted_winomt/decode.ini --range=1:1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMR3tcP9mNSZ"
      },
      "source": [
        "Once decoding is finished and `$decode_dir/out.text` exists, we  convert the output to plaintext for evaluation and evalute using WinoMT as before (if the entire test set has not been decoded, WinoMT will throw an error about  \"not enough values to unpack\".)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwxmDNA0Bu_I"
      },
      "source": [
        "%%bash  \n",
        "BASEDIR=/content/drive/My\\ Drive/gender-debias-walkthrough\n",
        "CONVERT_SCRIPT=\"$BASEDIR\"/scripts/convert_id_to_detok.sh\n",
        "WINOMT_PREP_SCRIPT=\"$BASEDIR\"/scripts/prep_for_winomt_eval.sh\n",
        "lang=de\n",
        "winomt_logs=/content/winomtout\n",
        "\n",
        "bash \"$CONVERT_SCRIPT\" $lang detok < decode_adapted_winomt/output.ids 1> decode_adapted_winomt/output.detok\n",
        "bash \"$WINOMT_PREP_SCRIPT\" decode_adapted_winomt/output.detok $lang /content/mt_gender\n",
        "cd mt_gender/src\n",
        "export FAST_ALIGN_BASE=/content/fast_align/\n",
        "bash ../scripts/evaluate_all_languages.sh ../data/aggregates/en.txt $winomt_logs &> $winomt_logs/adapted\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqtmGW2Qm4UQ"
      },
      "source": [
        "# The catastrophic forgetting problem\n",
        "\n",
        "Unfortuantely, a known downside of domain adaptation for neural models is that the models tend to experience `catastrophic forgetting' of anything they've seen previously.\n",
        "\n",
        "In a way, we're relying on this effect! We want the model to quickly forget the gender bias it has seen during baseline training. \n",
        "\n",
        "But the NMT model has also seen good, fluent examples of translation which let it translate well in general. We don't want forgetting to impact this ability.\n",
        "\n",
        "We can assess forgetting by measuring translation performance in [BLEU points](https://en.wikipedia.org/wiki/BLEU) on standard translation test sets. \n",
        "\n",
        "First we have to translate the test set just as we did for WinoMT."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BF8GqZsVBMQ9"
      },
      "source": [
        "To decode the general test set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URRILHcvl9eT"
      },
      "source": [
        "%%bash\n",
        "\n",
        "BASEDIR=/content/drive/My\\ Drive/gender-debias-walkthrough\n",
        "SRC_VOCAB_SIZE=$(wc -l < \"$BASEDIR/data_ende/handcrafted_ende/vocab.src\")\n",
        "TRG_VOCAB_SIZE=$(wc -l < \"$BASEDIR/data_ende/handcrafted_ende/vocab.trg\")\n",
        "T2T_USR_DIR=\"$BASEDIR/t2t-usr\"\n",
        "SRC_TEST=\"$BASEDIR/data_ende/test.ende.src.ids\"\n",
        "model_dir=model/ft/ende/\n",
        "decode_dir=decode_adapted\n",
        "mkdir -p $decode_dir\n",
        "\n",
        "config_file=$decode_dir/decode.ini\n",
        "echo \"verbosity: debug\n",
        "predictors: t2t\n",
        "src_test: \"$SRC_TEST\"\n",
        "decoder: beam\n",
        "beam: 4\n",
        "t2t_model: transformer\n",
        "t2t_hparams_set: transformer_base\n",
        "t2t_problem: translate_generic_existing_vocab\n",
        "pred_src_vocab_size: \"$SRC_VOCAB_SIZE\"\n",
        "pred_trg_vocab_size: \"$TRG_VOCAB_SIZE\"\n",
        "indexing_scheme: t2t\n",
        "t2t_usr_dir: \"$T2T_USR_DIR\"\n",
        "t2t_unk_id: 3\n",
        "output_path: \"$decode_dir\"/output.ids\n",
        "t2t_checkpoint_dir: \"$model_dir\"\n",
        "outputs: text\" > $config_file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9ibLLlzCYA6"
      },
      "source": [
        "%%bash \n",
        "source activate sgnmt_env\n",
        "BASEDIR=/content/drive/My\\ Drive/gender-debias-walkthrough\n",
        "T2T_USR_DIR=\"$BASEDIR/t2t-usr\"\n",
        "PYTHONPATH=$T2T_USR_DIR:$PYTHONPATH\n",
        "python /content/sgnmt/decode.py --config_file=decode_adapted/decode.ini --range=1:1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTx2Q6SvqtnA"
      },
      "source": [
        "When the test set is translated we evaluate it automatically against reference translations using a tool called sacreBLEU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbNgPcbamsDQ"
      },
      "source": [
        "%%bash  \n",
        "BASEDIR=/content/drive/My\\ Drive/gender-debias-walkthrough\n",
        "REF=\"$BASEDIR\"/data_ende/test.ende.trg.detok \n",
        "CONVERT_SCRIPT=\"$BASEDIR\"/scripts/convert_id_to_detok.sh\n",
        "lang=de\n",
        "bash \"$CONVERT_SCRIPT\" $lang detok < decode_adapted/output.ids 1> decode_adapted/output.detok\n",
        "## If you have decoded the entire test set, run sacrebleu against the entire reference\n",
        "head -1 \"$REF\" > ref.line1\n",
        "sacrebleu ref.line1 < decode_adapted/output.detok\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d30HyBsQplDG"
      },
      "source": [
        "If you decode and evaluate the test set with both the baseline and adapted models, you'll see that there's a noticeable drop in translation performance as measured by BLEU.\n",
        "\n",
        "One way around this is to constrain the adapted model to only produce gender-inflected versions of the baseline model's translation. This is a form of lattice rescoring. Essentially, it should ensure the adapted model can only deviate from the baseline translation to produce different gender inflections."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ozseu93sU-4"
      },
      "source": [
        "# Lattice rescoring: \"Correcting\" gender bias in translations\n",
        "\n",
        "Our alternative solution doesn't try to avoid forgetting when adapting the model. Instead, we fully embrace the forgetting, which gives good improvements on gender debiasing. But during decoding, we try to keep the translation close to a previously-generated translation (which may contain gender-bias-related mistakes.)\n",
        "\n",
        "In fact, we limit the translation model to produce only gender-inflected versions of the original translation.\n",
        "\n",
        "For example our baseline model translates the English sentence in WinoMT:\n",
        "\n",
        "*The cleaner hates **the developer** because **she** always leaves the room dirty.*\n",
        "\n",
        "to the German\n",
        "\n",
        "*Der Reiniger haßt **den Entwickler**, weil **er** den Raum immer schmutzig l&auml;sst.*\n",
        "\n",
        "Both bolded parts in the German sentence are (incorrectly) masculine forms. \n",
        "\n",
        "But we can use automatic tools with giant vocabulary lists to find that in German, *Entwickler* has an alternate inflected form *Entwicklerin*, and the masculine pronoun *er* has an equivalent feminine inflection *sie*. \n",
        "\n",
        "Of course, we don't know which words in the translation might be wrongly inflected, so we go through this process and find alternate inflections for every word in every translated sentence. We represent this series of alternate inflections as a lattice using OpenFST tools.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tc05JFO6tYCj"
      },
      "source": [
        "%%bash  \n",
        "BASEDIR=/content/drive/My\\ Drive/gender-debias-walkthrough\n",
        "INFLECTED_VOCAB_MAPPINGS=\"$BASEDIR/data_ende/inflections-de\"\n",
        "FST_SYMS=\"$BASEDIR/data_shared/fst_syms\"\n",
        "APPLY_BPE_SCRIPT=\"$BASEDIR/scripts/apply_bpe.sh\"\n",
        "SENTENCE_TO_FST=\"$BASEDIR/scripts/sentence_to_fst.py\"\n",
        "INFLECTIONS_TO_FST=\"$BASEDIR/scripts/multi_options_to_fst.py\"\n",
        "CONVERT_SCRIPT=\"$BASEDIR\"/scripts/convert_id_to_detok.sh\n",
        "lang=de\n",
        "bash \"$CONVERT_SCRIPT\" $lang < decode_adapted_winomt/output.ids 1> decode_adapted_winomt/output.tok\n",
        "\n",
        "plaintext_hyps=decode_adapted_winomt/output.tok\n",
        "sentence_count=$(wc -l < \"$plaintext_hyps\")\n",
        "\n",
        "lattice_dir=winomt_lattices.de\n",
        "mkdir -p $lattice_dir\n",
        "for i in $(seq $sentence_count); do\n",
        "    tmpfsttext=$lattice_dir/$i.tmp.fst\n",
        "    awk \"NR==$i\" $plaintext_hyps | bash \"$APPLY_BPE_SCRIPT\" $lang | python \"$SENTENCE_TO_FST\" \\\n",
        "    | fstcompile --isymbols=\"$FST_SYMS\"  --osymbols=\"$FST_SYMS\" > $tmpfsttext;\n",
        "    awk \"NR==$i\" $plaintext_hyps |  tr ' ' '\\n' | sed 's/^/ /g' | sort -u \\\n",
        "    | fgrep -f - \"$INFLECTED_VOCAB_MAPPINGS\" | bash \"$APPLY_BPE_SCRIPT\" $lang | sed 's/ 1 / | /g' \\\n",
        "    | python \"$INFLECTIONS_TO_FST\" | fstcompile --isymbols=\"$FST_SYMS\" --osymbols=\"$FST_SYMS\" | fstcompose $tmpfsttext - |  fstminimize --allow_nondet=true \\\n",
        "    > $lattice_dir/$i.fst\n",
        "    rm $tmpfsttext\n",
        "done\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsEj3iVTDBQY"
      },
      "source": [
        "When creating a config file for SGNMT decoding, we can now instruct the decoder to constrain search to the provided lattices using the \"predictors\" and \"fst_path\" options."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZhQcE0VzxSX"
      },
      "source": [
        "%%bash\n",
        "\n",
        "BASEDIR=/content/drive/My\\ Drive/gender-debias-walkthrough\n",
        "SRC_VOCAB_SIZE=$(wc -l < \"$BASEDIR/data_ende/handcrafted_ende/vocab.src\")\n",
        "TRG_VOCAB_SIZE=$(wc -l < \"$BASEDIR/data_ende/handcrafted_ende/vocab.trg\")\n",
        "T2T_USR_DIR=\"$BASEDIR/t2t-usr\"\n",
        "SRC_TEST=\"$BASEDIR/data_ende/winomt.ende.src.ids\"\n",
        "lattice_dir=winomt_lattices.de\n",
        "model_dir=model/ft/ende/\n",
        "decode_dir=decode_adapted_winomt.lattices\n",
        "mkdir -p $decode_dir\n",
        "\n",
        "config_file=$decode_dir/decode.ini\n",
        "echo \"verbosity: debug\n",
        "predictors: t2t,nfst\n",
        "src_test: \"$SRC_TEST\"\n",
        "decoder: beam\n",
        "beam: 4\n",
        "t2t_model: transformer\n",
        "t2t_hparams_set: transformer_base\n",
        "t2t_problem: translate_generic_existing_vocab\n",
        "pred_src_vocab_size: \"$SRC_VOCAB_SIZE\"\n",
        "pred_trg_vocab_size: \"$TRG_VOCAB_SIZE\"\n",
        "indexing_scheme: t2t\n",
        "t2t_usr_dir: \"$T2T_USR_DIR\"\n",
        "t2t_unk_id: 3\n",
        "output_path: \"$decode_dir\"/output.ids\n",
        "fst_path: \"$lattice_dir/%d.fst\"\n",
        "t2t_checkpoint_dir: \"$model_dir\"\n",
        "outputs: text\" > $config_file\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a5I6w-9x-h5"
      },
      "source": [
        "We can now decode with the debiased model, constraining it to produce only words in the lattice. Effectively it can only produce alternately-inflected forms of the original translation. Because the original, fluent baseline model gets control of the words in the translation, and the debiased model only changes inflections, we would hope that translation quality doesn't change much."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9aMuHXGBC70n"
      },
      "source": [
        "%%bash \n",
        "source activate sgnmt_env\n",
        "\n",
        "FSTPATH=/usr/local/lib/python3.6/dist-packages/openfst_python\n",
        "\n",
        "BASEDIR=/content/drive/My\\ Drive/gender-debias-walkthrough\n",
        "T2T_USR_DIR=\"$BASEDIR/t2t-usr\"\n",
        "PYTHONPATH=\"$FSTPATH:$T2T_USR_DIR:$PYTHONPATH\"\n",
        "\n",
        "python /content/sgnmt/decode.py --config_file=decode_adapted_winomt.lattices/decode.ini --range=1:1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQirsZe3z5wM"
      },
      "source": [
        "If we do this to the general test set, we shouldn't see much change in BLEU. If we do this to the WinoMT test set, we see almost as much reduction in gender bias as for the adapted model.\n",
        "\n",
        "One particularly good feature of lattice rescoring is that we don't actually need the baseline model at all - just its translation hypotheses. We can therefore also apply this lattice-rescoring method to the output of commercial translation systems as collected by Stanovsky et al, and reduce gender bias in those as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuoBpN9oBgnY"
      },
      "source": [
        "# Elastic Weight Consolidation: balancing gender debiasing and translation fluency\n",
        "\n",
        "An alternative way to deal with the catastrophic forgetting problem is [Elastic Weight Consolidation (EWC)](https://arxiv.org/abs/1612.00796). \n",
        "\n",
        "The general idea of EWC is to estimate how important different parameters in the neural network are to a task (general translation ability.) \n",
        "\n",
        "Then, when adapting the model to a new task (gender debiasing) we just apply a larger penalty to changing a parameter if the parameter was important for the previous task.\n",
        "\n",
        "EWC is not implemented in main Tensor2Tensor, so we first set up an environment for an older forked version with EWC implemented."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lm8pyEU97dC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29b50713-0078-41c3-ba4f-dd3da2a703b2"
      },
      "source": [
        "%%bash\n",
        "# make local copy of env file so conda doesn't attempt to write to drive\n",
        "cp \"/content/drive/My Drive/gender-debias-walkthrough/data_shared/env.yml\" /content/env.yml\n",
        "conda create -y -n tf_env_ewc pip python=2.7\n",
        "conda activate tf_env_ewc\n",
        "conda env update --file /content/env.yml   -n tf_env_ewc\n",
        "git clone --single-branch -b dsaunders_v1.4.3_modified-ewc  https://github.com/DCSaunders/tensor2tensor.git\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting package metadata (current_repodata.json): ...working... done\n",
            "Solving environment: ...working... failed with repodata from current_repodata.json, will retry with next repodata source.\n",
            "Collecting package metadata (repodata.json): ...working... done\n",
            "Solving environment: ...working... done\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local/envs/tf_env_ewc\n",
            "\n",
            "  added / updated specs:\n",
            "    - pip\n",
            "    - python=2.7\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    python-2.7.18              |       h15b4118_1         9.9 MB\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:         9.9 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  _libgcc_mutex      pkgs/main/linux-64::_libgcc_mutex-0.1-main\n",
            "  ca-certificates    pkgs/main/linux-64::ca-certificates-2021.1.19-h06a4308_1\n",
            "  certifi            pkgs/main/noarch::certifi-2020.6.20-pyhd3eb1b0_3\n",
            "  libffi             pkgs/main/linux-64::libffi-3.3-he6710b0_2\n",
            "  libgcc-ng          pkgs/main/linux-64::libgcc-ng-9.1.0-hdf63c60_0\n",
            "  libstdcxx-ng       pkgs/main/linux-64::libstdcxx-ng-9.1.0-hdf63c60_0\n",
            "  ncurses            pkgs/main/linux-64::ncurses-6.2-he6710b0_1\n",
            "  pip                pkgs/main/linux-64::pip-19.3.1-py27_0\n",
            "  python             pkgs/main/linux-64::python-2.7.18-h15b4118_1\n",
            "  readline           pkgs/main/linux-64::readline-8.1-h27cfd23_0\n",
            "  setuptools         pkgs/main/linux-64::setuptools-44.0.0-py27_0\n",
            "  sqlite             pkgs/main/linux-64::sqlite-3.35.1-hdfb4753_0\n",
            "  tk                 pkgs/main/linux-64::tk-8.6.10-hbc83047_0\n",
            "  wheel              pkgs/main/noarch::wheel-0.36.2-pyhd3eb1b0_0\n",
            "  zlib               pkgs/main/linux-64::zlib-1.2.11-h7b6447c_3\n",
            "\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "\rpython-2.7.18        | 9.9 MB    |            |   0% \rpython-2.7.18        | 9.9 MB    | ###        |  31% \rpython-2.7.18        | 9.9 MB    | ########## | 100% \rpython-2.7.18        | 9.9 MB    | ########## | 100% \n",
            "Preparing transaction: ...working... done\n",
            "Verifying transaction: ...working... done\n",
            "Executing transaction: ...working... done\n",
            "#\n",
            "# To activate this environment, use\n",
            "#\n",
            "#     $ conda activate tf_env_ewc\n",
            "#\n",
            "# To deactivate an active environment, use\n",
            "#\n",
            "#     $ conda deactivate\n",
            "\n",
            "Warning: you have pip-installed dependencies in your environment file, but you do not list pip itself as one of your conda dependencies.  Conda may not use the correct pip to install your packages, and they may end up in the wrong place.  Please add an explicit pip dependency.  I'm adding one for you, but still nagging you.\n",
            "Collecting package metadata (repodata.json): ...working... done\n",
            "Solving environment: ...working... done\n",
            "Preparing transaction: ...working... done\n",
            "Verifying transaction: ...working... done\n",
            "Executing transaction: ...working... done\n",
            "Installing pip dependencies: ...working... Ran pip subprocess with arguments:\n",
            "['/usr/local/envs/tf_env_ewc/bin/python', '-m', 'pip', 'install', '-U', '-r', '/content/condaenv.t4wipqcc.requirements.txt']\n",
            "Pip subprocess output:\n",
            "Processing /root/.cache/pip/wheels/18/ea/5e/e36e1b8739e78cd2eba0a08fdc602c2b16a4b263912af8cb64/absl_py-0.6.1-cp27-none-any.whl\n",
            "Collecting attrs==19.1.0\n",
            "  Using cached https://files.pythonhosted.org/packages/23/96/d828354fa2dbdf216eaa7b7de0db692f12c234f7ef888cc14980ef40d1d2/attrs-19.1.0-py2.py3-none-any.whl\n",
            "Requirement already up-to-date: backports-abc==0.5 in /usr/local/envs/tf_env_ewc/lib/python2.7/site-packages (from -r /content/condaenv.t4wipqcc.requirements.txt (line 3)) (0.5)\n",
            "Requirement already up-to-date: backports.functools-lru-cache==1.5 in /usr/local/envs/tf_env_ewc/lib/python2.7/site-packages (from -r /content/condaenv.t4wipqcc.requirements.txt (line 4)) (1.5)\n",
            "Requirement already up-to-date: backports.weakref==1.0.post1 in /tensorflow-1.15.2/python3.7 (from -r /content/condaenv.t4wipqcc.requirements.txt (line 5)) (1.0.post1)\n",
            "Collecting bioc==1.0\n",
            "  Using cached https://files.pythonhosted.org/packages/e0/21/3137c53520894b9d2ab04f267afcdaf6b7dd45ee38e724fa0585994eb6e9/bioc-1.0-py2.py3-none-any.whl\n",
            "Collecting bleach==1.5.0\n",
            "  Using cached https://files.pythonhosted.org/packages/33/70/86c5fec937ea4964184d4d6c4f0b9551564f821e1c3575907639036d9b90/bleach-1.5.0-py2.py3-none-any.whl\n",
            "Collecting blis==0.2.4\n",
            "  Using cached https://files.pythonhosted.org/packages/61/b7/6f32b1e2506937525802d94136eb73dec2cacd4a21c9bec9c90549e2b413/blis-0.2.4-cp27-cp27mu-manylinux1_x86_64.whl\n",
            "Collecting boto==2.49.0\n",
            "  Using cached https://files.pythonhosted.org/packages/23/10/c0b78c27298029e4454a472a1919bde20cb182dab1662cec7f2ca1dcc523/boto-2.49.0-py2.py3-none-any.whl\n",
            "Collecting boto3==1.10.0\n",
            "  Using cached https://files.pythonhosted.org/packages/f1/63/56d49c9ac4b00e56fa9a954e995cc08098b1adebb4fa5654e19ff734567d/boto3-1.10.0-py2.py3-none-any.whl\n",
            "Collecting botocore==1.13.0\n",
            "  Using cached https://files.pythonhosted.org/packages/f4/72/5d81c5eb90ce94e438875d166ed98724bbe24be1283ec650cdfe2107cd3e/botocore-1.13.0-py2.py3-none-any.whl\n",
            "Requirement already up-to-date: bz2file==0.98 in /tensorflow-1.15.2/python3.7 (from -r /content/condaenv.t4wipqcc.requirements.txt (line 12)) (0.98)\n",
            "Collecting chardet==3.0.4\n",
            "  Using cached https://files.pythonhosted.org/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl\n",
            "Collecting cymem==2.0.2\n",
            "  Using cached https://files.pythonhosted.org/packages/df/b1/4ff2cbd423184bd68e85f1daa6692753cd7710b0ba68552eb64542906a57/cymem-2.0.2-cp27-cp27mu-manylinux1_x86_64.whl\n",
            "Collecting dawg-python==0.7.2\n",
            "  Using cached https://files.pythonhosted.org/packages/6a/84/ff1ce2071d4c650ec85745766c0047ccc3b5036f1d03559fd46bb38b5eeb/DAWG_Python-0.7.2-py2.py3-none-any.whl\n",
            "Processing /root/.cache/pip/wheels/22/45/4f/a227f4774c4c267bc87adeb85aba7eebd2ead6df647a574842/distribute-0.7.3-cp27-none-any.whl\n",
            "Processing /root/.cache/pip/wheels/9b/04/dd/7daf4150b6d9b12949298737de9431a324d4b797ffd63f526e/docopt-0.6.2-py2.py3-none-any.whl\n",
            "Collecting docutils==0.14\n",
            "  Using cached https://files.pythonhosted.org/packages/50/09/c53398e0005b11f7ffb27b7aa720c617aba53be4fb4f4f3f06b9b5c60f28/docutils-0.14-py2-none-any.whl\n",
            "Collecting enum34==1.1.6\n",
            "  Using cached https://files.pythonhosted.org/packages/c5/db/e56e6b4bbac7c4a06de1c50de6fe1ef3810018ae11732a50f15f62c7d050/enum34-1.1.6-py2-none-any.whl\n",
            "Processing /root/.cache/pip/wheels/96/ff/1b/f3708731cf77a6d759cd12e68c0a27700c299fbe2bffd34fa3/fuel-0.2.0-cp27-cp27mu-linux_x86_64.whl\n",
            "Collecting funcsigs==1.0.2\n",
            "  Using cached https://files.pythonhosted.org/packages/69/cb/f5be453359271714c01b9bd06126eaf2e368f1fddfff30818754b5ac2328/funcsigs-1.0.2-py2.py3-none-any.whl\n",
            "Processing /root/.cache/pip/wheels/bf/c9/a3/c538d90ef17cf7823fa51fc701a7a7a910a80f6a405bf15b1a/future-0.16.0-cp27-none-any.whl\n",
            "Collecting gensim==3.8.1\n",
            "  Using cached https://files.pythonhosted.org/packages/ec/db/d0c6edd6e7211e7c47404034ed9dd71032a0a77c6ae8835505f1bd176a55/gensim-3.8.1-cp27-cp27mu-manylinux1_x86_64.whl\n",
            "Collecting graphviz==0.8.2\n",
            "  Using cached https://files.pythonhosted.org/packages/05/e4/8fcc76823534d47f079c0ff1b3d8b57784e8fba63ceb1ded32c9f4dd993c/graphviz-0.8.2-py2.py3-none-any.whl\n",
            "Collecting h5py==2.7.1\n",
            "  Using cached https://files.pythonhosted.org/packages/24/9e/d68bd01058e748bd5e7c3c6368d1703b4cd882b669e5d993a0237c75af5a/h5py-2.7.1-cp27-cp27mu-manylinux1_x86_64.whl\n",
            "Processing /root/.cache/pip/wheels/50/ae/f9/d2b189788efcf61d1ee0e36045476735c838898eef1cad6e29/html5lib-0.9999999-cp27-none-any.whl\n",
            "Collecting idna==2.6\n",
            "  Using cached https://files.pythonhosted.org/packages/27/cc/6dd9a3869f15c2edfab863b992838277279ce92663d334df9ecf5106f5c6/idna-2.6-py2.py3-none-any.whl\n",
            "Collecting jmespath==0.9.4\n",
            "  Using cached https://files.pythonhosted.org/packages/83/94/7179c3832a6d45b266ddb2aac329e101367fbdb11f425f13771d27f225bb/jmespath-0.9.4-py2.py3-none-any.whl\n",
            "Collecting jsonschema==3.0.1\n",
            "  Using cached https://files.pythonhosted.org/packages/aa/69/df679dfbdd051568b53c38ec8152a3ab6bc533434fc7ed11ab034bf5e82f/jsonschema-3.0.1-py2.py3-none-any.whl\n",
            "Collecting lda==1.1.0\n",
            "  Using cached https://files.pythonhosted.org/packages/7b/9f/4c429a39477333ca96a0a3a49de04e55756b6128be272c2600e31783f09a/lda-1.1.0-cp27-cp27mu-manylinux1_x86_64.whl\n",
            "Collecting lxml==4.2.5\n",
            "  Using cached https://files.pythonhosted.org/packages/e5/14/f4343239f955442da9da1919a99f7311bc5627522741bada61b2349c8def/lxml-4.2.5-cp27-cp27mu-manylinux1_x86_64.whl\n",
            "Collecting markdown==3.0.1\n",
            "  Using cached https://files.pythonhosted.org/packages/7a/6b/5600647404ba15545ec37d2f7f58844d690baf2f81f3a60b862e48f29287/Markdown-3.0.1-py2.py3-none-any.whl\n",
            "Requirement already up-to-date: mkl-fft==1.0.10 in /usr/local/envs/tf_env_ewc/lib/python2.7/site-packages (from -r /content/condaenv.t4wipqcc.requirements.txt (line 33)) (1.0.10)\n",
            "Requirement already up-to-date: mkl-random==1.0.2 in /usr/local/envs/tf_env_ewc/lib/python2.7/site-packages (from -r /content/condaenv.t4wipqcc.requirements.txt (line 34)) (1.0.2)\n",
            "Collecting mock==2.0.0\n",
            "  Using cached https://files.pythonhosted.org/packages/e6/35/f187bdf23be87092bd0f1200d43d23076cee4d0dec109f195173fd3ebc79/mock-2.0.0-py2.py3-none-any.whl\n",
            "Processing /root/.cache/pip/wheels/33/15/0f/9ca5f2ad88a5456803098daa189f382408a81556aa209e97ff/mpmath-1.0.0-cp27-none-any.whl\n",
            "Collecting murmurhash==1.0.2\n",
            "  Using cached https://files.pythonhosted.org/packages/ed/31/247b34db5ab06afaf5512481e77860fb4cd7a0c0ddff9d2566651c8c2f07/murmurhash-1.0.2-cp27-cp27mu-manylinux1_x86_64.whl\n",
            "Processing /root/.cache/pip/wheels/54/40/b7/c56ad418e6cd4d9e1e594b5e138d1ca6eec11a6ee3d464e5bb/nltk-3.4.3-cp27-none-any.whl\n",
            "Collecting numexpr==2.6.4\n",
            "  Using cached https://files.pythonhosted.org/packages/2e/5f/2853bd7a12eaced19a422acee23b36aa50074f3f4888133cdeda62a90c36/numexpr-2.6.4-cp27-cp27mu-manylinux1_x86_64.whl\n",
            "Processing /root/.cache/pip/wheels/c4/19/76/61fc7929d808e51567aff23036ca5fe6ba8336ad0559ca6a27/olefile-0.44-cp27-none-any.whl\n",
            "Processing /root/.cache/pip/wheels/f9/b2/4a/68efdfe5093638a9918bd1bb734af625526e849487200aa171/pathlib-1.0.1-cp27-none-any.whl\n",
            "Collecting pbr==3.1.1\n",
            "  Using cached https://files.pythonhosted.org/packages/0c/5d/b077dbf309993d52c1d71e6bf6fe443a8029ea215135ebbe0b1b10e7aefc/pbr-3.1.1-py2.py3-none-any.whl\n",
            "Processing /root/.cache/pip/wheels/36/dd/e2/ec30ef7c475e1d9fb966735984ba05f8710c67d7de5358c326/picklable_itertools-0.1.1-cp27-none-any.whl\n",
            "Collecting pillow==4.3.0\n",
            "  Using cached https://files.pythonhosted.org/packages/40/45/cd1000f1c474136236c5105c882d8e1e40bd94ae939b5ca53bf724967514/Pillow-4.3.0-cp27-cp27mu-manylinux1_x86_64.whl\n",
            "Collecting plac==0.9.6\n",
            "  Using cached https://files.pythonhosted.org/packages/9e/9b/62c60d2f5bc135d2aa1d8c8a86aaf84edb719a59c7f11a4316259e61a298/plac-0.9.6-py2.py3-none-any.whl\n",
            "Collecting preshed==2.0.1\n",
            "  Using cached https://files.pythonhosted.org/packages/25/b1/9098d07e70b960001a8a9b99435c6987006d0d7bcbf20523adce9272f66e/preshed-2.0.1-cp27-cp27mu-manylinux1_x86_64.whl\n",
            "Processing /root/.cache/pip/wheels/9b/39/68/3caa21b67a51d30549bcabc9a1500c1c19bad430cb6ec3ce3f/progressbar2-3.6.0-cp27-none-any.whl\n",
            "Collecting protobuf==3.6.1\n",
            "  Using cached https://files.pythonhosted.org/packages/b8/c2/b7f587c0aaf8bf2201405e8162323037fe8d17aa21d3c7dda811b8d01469/protobuf-3.6.1-cp27-cp27mu-manylinux1_x86_64.whl\n",
            "Processing /root/.cache/pip/wheels/6b/b9/15/c8c6a1e095a370e8c3273e65a5c982e5cf355dde16d77502f5/pyrsistent-0.15.2-cp27-cp27mu-linux_x86_64.whl\n",
            "Processing /root/.cache/pip/wheels/7b/16/48/5968d424cca21eb0215b5d98dddb5c09d3d23ef1efdacdfe47/pyter-0.2.2.1-cp27-none-any.whl\n",
            "Processing /root/.cache/pip/wheels/de/c2/93/660fd5f7559049268ad2dc6d81c4e39e9e36518766eaf7e342/python_Levenshtein-0.12.0-cp27-cp27mu-linux_x86_64.whl\n",
            "Processing /root/.cache/pip/wheels/7f/80/f3/ba284da14b9b8528fd957b3a82111f98cbe7310d2913fb514f/PyYAML-3.11-cp27-cp27mu-linux_x86_64.whl\n",
            "Collecting pyzmq==16.0.3\n",
            "  Using cached https://files.pythonhosted.org/packages/42/5a/95d2bf05e72a16c3f371457e07a3f6920bd6766d5a9648c1c70333b07dc8/pyzmq-16.0.3-cp27-cp27mu-manylinux1_x86_64.whl\n",
            "Collecting requests==2.18.4\n",
            "  Using cached https://files.pythonhosted.org/packages/49/df/50aa1999ab9bde74656c2919d9c0c085fd2b3775fd3eca826012bef76d8c/requests-2.18.4-py2.py3-none-any.whl\n",
            "Collecting s3transfer==0.2.1\n",
            "  Using cached https://files.pythonhosted.org/packages/16/8a/1fc3dba0c4923c2a76e1ff0d52b305c44606da63f718d14d3231e21c51b0/s3transfer-0.2.1-py2.py3-none-any.whl\n",
            "Collecting scikit-learn==0.20.3\n",
            "  Using cached https://files.pythonhosted.org/packages/f7/bb/52a01390c1dbb2c65d3072bc687271aa9ddf6964141ce7e03304820138f4/scikit_learn-0.20.3-cp27-cp27mu-manylinux1_x86_64.whl\n",
            "Collecting scipy==0.19.1\n",
            "  Using cached https://files.pythonhosted.org/packages/8e/43/a7b400e7ea07220fb419f0669ff17f5ef71653cf32827315224bc9dda9d4/scipy-0.19.1-cp27-cp27mu-manylinux1_x86_64.whl\n",
            "Processing /root/.cache/pip/wheels/5f/ea/fb/5b1a947b369724063b2617011f1540c44eb00e28c3d2ca8692/smart_open-1.8.4-cp27-none-any.whl\n",
            "Collecting srsly==0.0.6\n",
            "  Using cached https://files.pythonhosted.org/packages/f6/b5/1e40d0b8958a87cbb4b500535953f6a9f82054e63af473d66533d0494ac9/srsly-0.0.6-cp27-cp27mu-manylinux1_x86_64.whl\n",
            "Processing /root/.cache/pip/wheels/6d/47/7c/40a7cd9b9bd5bad329fcd21d8e50629700fcc6e5520a66a2de/sympy-1.1.1-cp27-none-any.whl\n",
            "Collecting tables==3.4.2\n",
            "  Using cached https://files.pythonhosted.org/packages/b4/70/57b9c64506fb2dd9ac24a61a32901ba8c6ba86bfd6017cc0096d3f8f1872/tables-3.4.2-3-cp27-cp27mu-manylinux1_x86_64.whl\n",
            "Collecting tensor2tensor==1.3.1\n",
            "  Using cached https://files.pythonhosted.org/packages/2d/fc/3bc5b1abe9cfd96a1c234fd43bed64a24a173c0cc67af2d4fbebd5b0d16e/tensor2tensor-1.3.1-py2.py3-none-any.whl\n",
            "Collecting tensorflow==1.5.0\n",
            "  Using cached https://files.pythonhosted.org/packages/69/6d/09d4fbeedbafbc6768a94901f14ace4153adba4c2e2c6e6f2080f4a5d1a7/tensorflow-1.5.0-cp27-cp27mu-manylinux1_x86_64.whl\n",
            "Collecting tensorflow-tensorboard==1.5.1\n",
            "  Using cached https://files.pythonhosted.org/packages/cd/ba/d664f7c27c710063b1cdfa0309db8fba98952e3a1ba1991ed98efffe69ed/tensorflow_tensorboard-1.5.1-py2-none-any.whl\n",
            "Collecting thinc==7.0.4\n",
            "  Using cached https://files.pythonhosted.org/packages/a1/26/9e80f304ea5924951831f7dccf3700c8f1047e9b15d1558830e1009134df/thinc-7.0.4-cp27-cp27mu-manylinux1_x86_64.whl\n",
            "Processing /root/.cache/pip/wheels/41/a9/4b/7e56c2682088aafa8f92cf8cb4825051b5ec34ac0ad0cb8054/toolz-0.7.2-cp27-none-any.whl\n",
            "Collecting tqdm==4.32.1\n",
            "  Using cached https://files.pythonhosted.org/packages/45/af/685bf3ce889ea191f3b916557f5677cc95a5e87b2fa120d74b5dd6d049d0/tqdm-4.32.1-py2.py3-none-any.whl\n",
            "Collecting urllib3==1.22\n",
            "  Using cached https://files.pythonhosted.org/packages/63/cb/6965947c13a94236f6d4b8223e21beb4d576dc72e8130bd7880f600839b8/urllib3-1.22-py2.py3-none-any.whl\n",
            "Processing /root/.cache/pip/wheels/b3/2c/d1/78fd1255da73ff77b372ecc56bcdb15115ab0882bb6f67af17/wasabi-0.2.2-cp27-none-any.whl\n",
            "Collecting werkzeug==0.14.1\n",
            "  Using cached https://files.pythonhosted.org/packages/20/c4/12e3e56473e52375aa29c4764e70d1b8f3efa6682bef8d0aae04fe335243/Werkzeug-0.14.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/envs/tf_env_ewc/lib/python2.7/site-packages (from absl-py==0.6.1->-r /content/condaenv.t4wipqcc.requirements.txt (line 1)) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.15.0 in /usr/local/envs/tf_env_ewc/lib/python2.7/site-packages (from blis==0.2.4->-r /content/condaenv.t4wipqcc.requirements.txt (line 8)) (1.15.4)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/envs/tf_env_ewc/lib/python2.7/site-packages (from botocore==1.13.0->-r /content/condaenv.t4wipqcc.requirements.txt (line 11)) (2.7.5)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=0.7 in /usr/local/envs/tf_env_ewc/lib/python2.7/site-packages (from distribute==0.7.3->-r /content/condaenv.t4wipqcc.requirements.txt (line 16)) (44.0.0.post20200106)\n",
            "Requirement already satisfied, skipping upgrade: functools32; python_version < \"3\" in /usr/local/envs/tf_env_ewc/lib/python2.7/site-packages (from jsonschema==3.0.1->-r /content/condaenv.t4wipqcc.requirements.txt (line 29)) (3.2.3.post2)\n",
            "Requirement already satisfied, skipping upgrade: singledispatch; python_version < \"3.4\" in /usr/local/envs/tf_env_ewc/lib/python2.7/site-packages (from nltk==3.4.3->-r /content/condaenv.t4wipqcc.requirements.txt (line 38)) (3.4.0.3)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/envs/tf_env_ewc/lib/python2.7/site-packages (from requests==2.18.4->-r /content/condaenv.t4wipqcc.requirements.txt (line 54)) (2020.6.20)\n",
            "Requirement already satisfied, skipping upgrade: futures<4.0.0,>=2.2.0; python_version == \"2.6\" or python_version == \"2.7\" in /usr/local/envs/tf_env_ewc/lib/python2.7/site-packages (from s3transfer==0.2.1->-r /content/condaenv.t4wipqcc.requirements.txt (line 55)) (3.2.0)\n",
            "Requirement already satisfied, skipping upgrade: wheel in /usr/local/envs/tf_env_ewc/lib/python2.7/site-packages (from tensorflow==1.5.0->-r /content/condaenv.t4wipqcc.requirements.txt (line 63)) (0.36.2)\n",
            "Installing collected packages: enum34, absl-py, attrs, lxml, docutils, bioc, html5lib, bleach, blis, boto, jmespath, urllib3, botocore, s3transfer, boto3, chardet, cymem, dawg-python, distribute, docopt, numexpr, tables, pyyaml, pyzmq, progressbar2, scipy, olefile, pillow, picklable-itertools, idna, requests, h5py, fuel, funcsigs, future, smart-open, gensim, graphviz, pyrsistent, jsonschema, pbr, lda, markdown, mock, mpmath, murmurhash, nltk, pathlib, plac, preshed, protobuf, pyter, python-levenshtein, scikit-learn, srsly, sympy, tensor2tensor, werkzeug, tensorflow-tensorboard, tensorflow, tqdm, wasabi, thinc, toolz\n",
            "Successfully installed absl-py-0.6.1 attrs-19.1.0 bioc-1.0 bleach-1.5.0 blis-0.2.4 boto-2.49.0 boto3-1.10.0 botocore-1.13.0 chardet-3.0.4 cymem-2.0.2 dawg-python-0.7.2 distribute-0.7.3 docopt-0.6.2 docutils-0.14 enum34-1.1.6 fuel-0.2.0 funcsigs-1.0.2 future-0.16.0 gensim-3.8.1 graphviz-0.8.2 h5py-2.7.1 html5lib-0.9999999 idna-2.6 jmespath-0.9.4 jsonschema-3.0.1 lda-1.1.0 lxml-4.2.5 markdown-3.0.1 mock-2.0.0 mpmath-1.0.0 murmurhash-1.0.2 nltk-3.4.3 numexpr-2.6.4 olefile-0.44 pathlib-1.0.1 pbr-3.1.1 picklable-itertools-0.1.1 pillow-4.3.0 plac-0.9.6 preshed-2.0.1 progressbar2-3.6.0 protobuf-3.6.1 pyrsistent-0.15.2 pyter-0.2.2.1 python-levenshtein-0.12.0 pyyaml-3.11 pyzmq-16.0.3 requests-2.18.4 s3transfer-0.2.1 scikit-learn-0.20.3 scipy-0.19.1 smart-open-1.8.4 srsly-0.0.6 sympy-1.1.1 tables-3.4.2 tensor2tensor-1.3.1 tensorflow-1.5.0 tensorflow-tensorboard-1.5.1 thinc-7.0.4 toolz-0.7.2 tqdm-4.32.1 urllib3-1.22 wasabi-0.2.2 werkzeug-0.14.1\n",
            "\n",
            "done\n",
            "#\n",
            "# To activate this environment, use\n",
            "#\n",
            "#     $ conda activate tf_env_ewc\n",
            "#\n",
            "# To deactivate an active environment, use\n",
            "#\n",
            "#     $ conda deactivate\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "CommandNotFoundError: Your shell has not been properly configured to use 'conda activate'.\n",
            "To initialize your shell, run\n",
            "\n",
            "    $ conda init <SHELL_NAME>\n",
            "\n",
            "Currently supported shells are:\n",
            "  - bash\n",
            "  - fish\n",
            "  - tcsh\n",
            "  - xonsh\n",
            "  - zsh\n",
            "  - powershell\n",
            "\n",
            "See 'conda init --help' for more information and options.\n",
            "\n",
            "IMPORTANT: You may need to close and restart your shell after running 'conda init'.\n",
            "\n",
            "\n",
            "fatal: destination path 'tensor2tensor' already exists and is not an empty directory.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ow0Vp49G-1Ah"
      },
      "source": [
        "Using this version of T2T we run adaptation to the same handcrafted set, now applying a loss penalty to the pre-saved EWC fisher variables:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-T5tvaK8r2RY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9656c60f-0498-452b-feec-7fddd7cf9e26"
      },
      "source": [
        "%%bash \n",
        "#tmppythonpath=$PYTHONPATH\n",
        "source activate tf_env_ewc\n",
        "BASEDIR='/content/drive/My Drive/gender-debias-walkthrough/'\n",
        "SRC_DIR=\"$BASEDIR/baseline_ende\"\n",
        "EWC_VARS=\"$BASEDIR/baseline_ende/ewc_vars\"\n",
        "\n",
        "DATA_DIR=\"$BASEDIR/data_ende/handcrafted_ende\"\n",
        "\n",
        "BASE_STEPS=300000\n",
        "adapt_steps=4\n",
        "TRAIN_STEPS==$(( $BASE_STEPS + $adapt_steps ))\n",
        "batch_size=4096\n",
        "ewc_loss_weight=10000000\n",
        "T2T=\"/content/tensor2tensor/\"\n",
        "T2T_USR_DIR=\"$BASEDIR/t2t-usr\"\n",
        "TF=\"/usr/local/lib/python3.6/site-packages/tensorflow/\"\n",
        "DATA_DIR=\"$BASEDIR/data_ende/handcrafted_ende\"\n",
        "PYTHONPATH=/content/tensor2tensor:/usr/local/lib/python2.7/site-packages:$T2T_USR_DIR:$PYTHONPATH\n",
        "\n",
        "#PYTHONPATH=$T2T_USR_DIR:$PYTHONPATH\n",
        "model_dir=model/ft/ende_ewc/\n",
        "mkdir -p $model_dir\n",
        "\n",
        "# make a local copy of the model\n",
        "cp \"$SRC_DIR\"/model* $model_dir/\n",
        "cp \"$SRC_DIR\"/checkpoint $model_dir/\n",
        "ln -s \"$EWC_VARS\" \"$model_dir/ewc_vars\"\n",
        "ls $model_dir\n",
        "python /content/tensor2tensor/tensor2tensor/bin/t2t_trainer.py \\\n",
        " --data_dir=\"$DATA_DIR\" \\\n",
        " --problems=translate_generic_existing_vocab --hparams_set=transformer_base \\\n",
        " --output_dir=$model_dir --model=transformer \\\n",
        " --schedule=train --train_steps=300004 --keep_checkpoint_max=1  \\\n",
        "--hparams=\"batch_size=$batch_size,ewc_load_vars=True,ewc_loss_weight=$ewc_loss_weight\"\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "checkpoint\n",
            "ewc_vars\n",
            "flags_t2t.txt\n",
            "flags.txt\n",
            "hparams.json\n",
            "model.ckpt-300000.data-00000-of-00002\n",
            "model.ckpt-300000.data-00001-of-00002\n",
            "model.ckpt-300000.index\n",
            "model.ckpt-300000.meta\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "ln: failed to create symbolic link 'model/ft/ende_ewc//ewc_vars': File exists\n",
            "/usr/local/envs/tf_env_ewc/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
            "  from ._conv import register_converters as _register_converters\n",
            "INFO:tensorflow:schedule=train\n",
            "INFO:tensorflow:worker_gpu=1\n",
            "INFO:tensorflow:sync=False\n",
            "INFO:tensorflow:datashard_devices: ['/job:localhost']\n",
            "INFO:tensorflow:caching_devices: None\n",
            "INFO:tensorflow:ps_devices: ['gpu:0']\n",
            "INFO:tensorflow:Using config: {'_save_checkpoints_secs': None, '_keep_checkpoint_max': 1, '_task_type': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f4121a9ce90>, '_keep_checkpoint_every_n_hours': 10000, '_session_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 0.95\n",
            "}\n",
            "allow_soft_placement: true\n",
            "graph_options {\n",
            "  optimizer_options {\n",
            "  }\n",
            "}\n",
            ", 'use_tpu': False, '_tf_random_seed': 1234, '_num_worker_replicas': 0, '_task_id': 0, 't2t_device_info': {'num_async_replicas': 1}, '_evaluation_master': '', '_log_step_count_steps': 100, '_num_ps_replicas': 0, '_is_chief': True, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_save_checkpoints_steps': 1000, '_environment': 'local', '_master': '', '_model_dir': 'model/ft/ende_ewc/', 'data_parallelism': <tensor2tensor.utils.expert_utils.Parallelism object at 0x7f4126d886d0>, '_save_summary_steps': 100}\n",
            "WARNING:tensorflow:Estimator's model_fn (<function wrapping_model_fn at 0x7f4121aa2488>) includes params argument, but params are not passed to Estimator.\n",
            "INFO:tensorflow:Reading data files from /content/drive/My Drive/gender-debias-walkthrough//data_ende/handcrafted_ende/translate_generic_existing_vocab-train*\n",
            "INFO:tensorflow:partition: 0 num_data_files: 1\n",
            "INFO:tensorflow:Setting T2TModel mode to 'train'\n",
            "INFO:tensorflow:Using variable initializer: uniform_unit_scaling\n",
            "INFO:tensorflow:Transforming feature 'inputs' with symbol_modality_35627_512.bottom\n",
            "INFO:tensorflow:Transforming 'targets' with symbol_modality_35627_512.targets_bottom\n",
            "INFO:tensorflow:Building model body\n",
            "WARNING:tensorflow:From /content/tensor2tensor/tensor2tensor/layers/common_layers.py:512: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "keep_dims is deprecated, use keepdims instead\n",
            "INFO:tensorflow:Transforming body output with symbol_modality_35627_512.top\n",
            "WARNING:tensorflow:From /content/tensor2tensor/tensor2tensor/layers/common_layers.py:1709: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
            "\n",
            "INFO:tensorflow:Base learning rate: 0.200000\n",
            "INFO:tensorflow:Applying exp learning rate warmup for 8000 steps\n",
            "INFO:tensorflow:Applying learning rate decay: noam.\n",
            "INFO:tensorflow:Trainable Variables Total size: 62361088\n",
            "INFO:tensorflow:Using optimizer Adam\n",
            "INFO:tensorflow:Adding EWC penalty to loss with lambda 10000000.0\n",
            "/usr/local/envs/tf_env_ewc/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py:97: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "2021-03-18 15:41:14.714047: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
            "INFO:tensorflow:Restoring parameters from model/ft/ende_ewc/model.ckpt-300000\n",
            "tcmalloc: large alloc 1137123328 bytes == 0x5598829ea000 @  0x7f4153f7f615 0x7f414ff2ee0d 0x7f4153c502be 0x7f4153c50dac 0x7f4153c50dac 0x7f4153c50dac 0x7f4153c50dac 0x7f4153c50dac 0x7f4153c50dac 0x7f4153c50dac 0x7f4153c50dac 0x7f4153c50dac 0x7f4153c50dac 0x7f4153c50dac 0x7f4153c50dac 0x7f4153c50dac 0x7f4153c50dac 0x7f4153c50dac 0x7f4153c50dac 0x7f4153c50dac 0x7f4153c50dac 0x7f4153c50dac 0x7f4153c50dac 0x7f4153c524e9 0x7f4153c4f482 0x7f4153c524e9 0x7f4153c4f482 0x7f4153c524e9 0x7f4153c4f482 0x7f4153c524e9 0x7f4153c4f482\n",
            "tcmalloc: large alloc 1421410304 bytes == 0x5599076bc000 @  0x7f4153f7f615 0x7f414ff2ee0d 0x7f4153c502be 0x7f4153c50dac 0x7f4153c50dac 0x7f4153c50dac 0x7f4153c50dac 0x7f4153c50dac 0x7f4153c50dac 0x7f4153c50dac 0x7f4153c50dac 0x7f4153c50dac 0x7f4153c50dac 0x7f4153c50dac 0x7f4153c50dac 0x7f4153c50dac 0x7f4153c50dac 0x7f4153c50dac 0x7f4153c50dac 0x7f4153c50dac 0x7f4153c50dac 0x7f4153c50dac 0x7f4153c50dac 0x7f4153c524e9 0x7f4153c4f482 0x7f4153c524e9 0x7f4153c4f482 0x7f4153c524e9 0x7f4153c4f482 0x7f4153c524e9 0x7f4153c4f482\n",
            "tcmalloc: large alloc 1776762880 bytes == 0x5598829ea000 @  0x7f4153f7f615 0x7f414ff2ee0d 0x7f4153c502be 0x7f4153c50dac 0x7f4153c50dac 0x7f4153c50dac 0x7f4153c50dac 0x7f4153c50dac 0x7f4153c50dac 0x7f4153c50dac 0x7f4153c50dac 0x7f4153c50dac 0x7f4153c50dac 0x7f4153c50dac 0x7f4153c524e9 0x7f4153c4f482 0x7f4153c524e9 0x7f4153c4f482 0x7f4153c524e9 0x7f4153c4f482 0x7f4153c524e9 0x7f4153c4f482 0x7f4153c524e9 0x7f4153bdb0c7 0x7f4153bb6773 0x7f4153c4b4d0 0x7f4153c524e9 0x7f4153c4f482 0x7f4153c524e9 0x7f4153c4f482 0x7f4153c524e9\n",
            "INFO:tensorflow:Saving checkpoints for 300001 into model/ft/ende_ewc/model.ckpt.\n",
            "INFO:tensorflow:loss = 1.4377266, step = 300001\n",
            "INFO:tensorflow:Saving checkpoints for 300004 into model/ft/ende_ewc/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.71776944.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Duyx2vLdsBS3"
      },
      "source": [
        "Although EWC reduces catastrophic forgetting, it also results in some trade-off between reduced gender bias and general translation ability. It also cannot be applied to black-box translations like lattice rescoring can. However, it does involve only a single model and decoding pass."
      ]
    }
  ]
}